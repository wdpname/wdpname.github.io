<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>迁移学习研究进展 | 平博社</title><meta name="keywords" content="迁移学习"><meta name="author" content="dpWu"><meta name="copyright" content="dpWu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="迁移学习：是运用已存有的知识对不同但相关领域问题进行求解的一种新的机器学习方法 传统分类学习中，都有两个基本的假设:(1) 用于学习的训练样本与新的测试样本满足独立同分布的条件;(2) 必须有足够可利用的训练样本才能学习得到一个好的分类模型.但是,在实际应用中我们发现,这两个条件往往无法满足. 数据分类首先要解决训练集样本抽样问题,如何抽到具有代表性的样本集作为训练集是一个值得研究的重要问题.文">
<meta property="og:type" content="article">
<meta property="og:title" content="迁移学习研究进展">
<meta property="og:url" content="https://wdpname.github.io/2022/12/19/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/index.html">
<meta property="og:site_name" content="平博社">
<meta property="og:description" content="迁移学习：是运用已存有的知识对不同但相关领域问题进行求解的一种新的机器学习方法 传统分类学习中，都有两个基本的假设:(1) 用于学习的训练样本与新的测试样本满足独立同分布的条件;(2) 必须有足够可利用的训练样本才能学习得到一个好的分类模型.但是,在实际应用中我们发现,这两个条件往往无法满足. 数据分类首先要解决训练集样本抽样问题,如何抽到具有代表性的样本集作为训练集是一个值得研究的重要问题.文">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-12-19T15:13:12.000Z">
<meta property="article:modified_time" content="2023-01-21T07:51:33.086Z">
<meta property="article:author" content="dpWu">
<meta property="article:tag" content="迁移学习">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/assets/avatar.jpeg"><link rel="canonical" href="https://wdpname.github.io/2022/12/19/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '迁移学习研究进展',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-21 15:51:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">121</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">平博社</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">迁移学习研究进展</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-19T15:13:12.000Z" title="发表于 2022-12-19 23:13:12">2022-12-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-21T07:51:33.086Z" title="更新于 2023-01-21 15:51:33">2023-01-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="迁移学习研究进展"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><ul>
<li>迁移学习：是运用已存有的知识对不同但相关领域问题进行求解的一种新的机器学习方法</li>
<li>传统分类学习中，都有两个基本的假设:(1) 用于学习的训练样本与新的测试样本满足独立同分布的条件;(2) 必须有足够可利用的训练样本才能学习得到一个好的分类模型.但是,在实际应用中我们发现,这两个条件往往无法满足.</li>
<li>数据分类首先要解决训练集样本抽样问题,如何抽到具有代表性的样本集作为训练集是一个值得研究的重要问题.文献[1]提出极小样本集抽样方法用于基于超曲面分类算法,该方法可感知非结构化数据的分布,并以极小样本集作为代表子集.该文还指出了极小样本集有多少种表达方式,给出了样本缺失情况下准确率的精确估计，并不是很多算法都能实现模型的高可靠性和高准确性。因此研究迁移学习很重要</li>
</ul>
<h1 id="迁移学习算法研究进展"><a href="#迁移学习算法研究进展" class="headerlink" title="迁移学习算法研究进展"></a>迁移学习算法研究进展</h1><h2 id="针对源领域和目标领域样本是否标注以及任务是否相同划分"><a href="#针对源领域和目标领域样本是否标注以及任务是否相同划分" class="headerlink" title="针对源领域和目标领域样本是否标注以及任务是否相同划分"></a>针对源领域和目标领域样本是否标注以及任务是否相同划分</h2><ul>
<li>归纳迁移学习 inductive transfer learning 一般我们传统的学习就是归纳学习，就是在训练中不使用测试集的数据来训练。</li>
<li>直推式迁移学习：这个和归纳迁移学习相反，我在训练的时候使用到了测试集中的有标签的数据样本。</li>
<li>无监督迁移学习</li>
<li>等</li>
</ul>
<h3 id="根据源领域和目标领域中是否有标签样本划分"><a href="#根据源领域和目标领域中是否有标签样本划分" class="headerlink" title="根据源领域和目标领域中是否有标签样本划分"></a>根据源领域和目标领域中是否有标签样本划分</h3><ul>
<li>目标领域中有少量标注样本的归纳迁移学习</li>
<li>只有源领域中有标签样本的直推式迁移学习</li>
<li>源领域和目标领域都没有标签样本的无监督迁移学习</li>
</ul>
<h3 id="根据源领域中是否有标签样本"><a href="#根据源领域中是否有标签样本" class="headerlink" title="根据源领域中是否有标签样本"></a>根据源领域中是否有标签样本</h3><p>归纳迁移学习 分为  多任务学习  自学习</p>
<h3 id="根据训练样本和测试样本是否来自于同一个领域"><a href="#根据训练样本和测试样本是否来自于同一个领域" class="headerlink" title="根据训练样本和测试样本是否来自于同一个领域"></a>根据训练样本和测试样本是否来自于同一个领域</h3><p>可以把直推式迁移学习划分为样本选择偏差、协方差偏移和领域自适应学习这些相关的子领域</p>
<h2 id="按照迁移学习方法采用的技术划分"><a href="#按照迁移学习方法采用的技术划分" class="headerlink" title="按照迁移学习方法采用的技术划分"></a>按照迁移学习方法采用的技术划分</h2><ul>
<li>基于特征选择的迁移学习算法研究</li>
<li>基于特征映射的迁移学习算法研究</li>
<li>基于权重的迁移学习算法研究</li>
</ul>
<h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><ul>
<li><p>能够利用少量有标签数据和大量没有标签样本数据的技术有如下：</p>
<ul>
<li><p>半监督学习</p>
<ul>
<li>指学习算法在学习过程中无需人工干预,基于自身对无标签数据加以利用</li>
<li>多视角学习(multi-view learning)也是半监督学习一个很重要的学习任务.可以提高半监督学习的算法的性能。</li>
</ul>
</li>
<li><p>直推式学习</p>
<ul>
<li><p>直推式学习与半监督学习一样也</p>
<p>无需人工干预,所不同的是,直推式学习假设无标签的数据就是最终要用来测试的数据,学习的目的就是在这些</p>
<p>数据上取得最佳泛化能力.相对应地,半监督学习在学习时并不知道最终的测试用例是什么.因此,半监督学习</p>
<p>考虑的是一个“开放的世界”,即在学习中不知道测试样本是什么,而直推式学习考虑的则是一个“封闭世界”,要</p>
<p>测试的样本数据已参与到学习过程中.如果抛开是否对未知样本进行预测,直推式学习可以归结为半监督学习</p>
<p>的一种特例</p>
<p>指原任务和目标任务相同，但原领域和目标领域不同</p>
</li>
</ul>
</li>
<li><p>主动学习</p>
<ul>
<li>主动学习与半监督学习、直推式学习最大的区别在于它的学习过程需要人工干预, 就是在学习过程中通过反馈尽可能地找到那些包含信息量大的样本来辅助少量有标签样本的学习</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="传统学习和各种迁移学习的关系"><a href="#传统学习和各种迁移学习的关系" class="headerlink" title="传统学习和各种迁移学习的关系"></a>传统学习和各种迁移学习的关系</h1><p>迁移学习是和传统学习相对应的一大类学习方式,传统学习处理源领域和目标领域相同且源领域和目标领域的任务是相同的学习,迁移学习处理除此情形之外的学习,</p>
<p>包括: （有可能下面这个解释好像是错误的，我是渣渣</p>
<p><strong>归纳迁移学习</strong>：源领域和目标领域的任务相关但不同的归纳迁移学习，</p>
<p><strong>直推式迁移学习</strong>：源领域和目标领域相关但不相同而源领域和目标领域的任务相同的直推式迁移学习，</p>
<p><strong>无监督迁移学习</strong>：无监督迁移学习与归纳迁移学习类似,不过,无监督迁移学习主要处理源领域和目标领域中都没有标签数据的问题</p>
<h1 id="a-comprehensive-survey-on-transfer-learning"><a href="#a-comprehensive-survey-on-transfer-learning" class="headerlink" title="a comprehensive survey on transfer learning"></a>a comprehensive survey on transfer learning</h1><h2 id="理想的机器学习场景"><a href="#理想的机器学习场景" class="headerlink" title="理想的机器学习场景"></a>理想的机器学习场景</h2><ul>
<li>有着充足的带有标签的训练样本，且其分布与测试样本相同</li>
</ul>
<h2 id="根据领域之间的差异："><a href="#根据领域之间的差异：" class="headerlink" title="根据领域之间的差异："></a>根据领域之间的差异：</h2><ul>
<li><p>同质迁移学习</p>
<ul>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212211523401.png" alt="image-20221221152300319"></li>
<li>域之间具有相同特征空间</li>
<li>假设仅边缘分布不同</li>
<li>但是为了解决上下文特征偏差这种现象，一些研究调整了条件分布</li>
</ul>
</li>
<li><p>异质迁移学习</p>
<ul>
<li>域之间具有不同特征空间</li>
<li>除了分布适应之外，异质迁移学习还需要特征空间适应[7]，这使得它比同质迁移学习更复杂。</li>
</ul>
</li>
</ul>
<p>剩余的研究分为7个部分</p>
<ul>
<li>第2节阐明了迁移学习和其他相关机器学习技术之间的区别</li>
<li>第3节介绍了本调查中使用的符号和关于迁移学习的定义。</li>
<li>第4节和第5节分别从数据和模型角度解释迁移学习方法</li>
<li>第6节介绍迁移学习的一些应用</li>
<li>进行了实验，结果见第7节。</li>
<li>最后一节总结了本次调查</li>
</ul>
<h2 id="2-related-work"><a href="#2-related-work" class="headerlink" title="2. related work"></a>2. related work</h2><h3 id="半监督学习-1"><a href="#半监督学习-1" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>已标记样本和未标记样本都来自<strong>相同的分布</strong></p>
<h3 id="多视图学习"><a href="#多视图学习" class="headerlink" title="多视图学习"></a>多视图学习</h3><p>有几种策略：</p>
<ul>
<li>子空间学习</li>
<li>多核学习</li>
<li>联合训练</li>
</ul>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>转移学习与多任务学习的主要区别：在于前者转移包含在相关领域中的知识，而后者通过同时学习一些相关任务来转移知识。</p>
<p>共同点：采用了一些类似的策略来构建模型，例如特征转换和参数共享。</p>
<h2 id="3-Overview"><a href="#3-Overview" class="headerlink" title="3.Overview"></a>3.Overview</h2><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212212131468.png" alt="image-20221221153146001"></p>
<p>符号定义表</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212212131821.png" alt="image-20221221153210378"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212211534166.png" alt="image-20221221153402067"></p>
<p>​    Categorizations of transfer learning.</p>
<h3 id="3-3-Categorization-of-Transfer-Learning"><a href="#3-3-Categorization-of-Transfer-Learning" class="headerlink" title="3.3. Categorization of Transfer Learning"></a>3.3. Categorization of Transfer Learning</h3><p>根据这篇文献S.J. Pan and Q. Yang, “A survey on transfer learning,” IEEE T rans.Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.， 可以将迁移学习方法分类为4个大类：</p>
<ul>
<li>基于样本的<ul>
<li>主要基于instace-weighting strategy</li>
</ul>
</li>
<li>基于特征的<ul>
<li>又细分为两种<ul>
<li>基于非对称特征的迁移学习<ul>
<li>非对称方法转变源特征来匹配目标特征</li>
</ul>
</li>
<li>基于对称特征的迁移学习<ul>
<li>对称方法试图找到一个共同的潜在特征空间，然后将源特征和目标特征转换为新的特征表示</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>基于参数的<ul>
<li>基于参数的转移学习方法在模型/参数级别迁移知识</li>
</ul>
</li>
<li>基于相关性的<ul>
<li>主要关注相关领域中的问题</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212211556526.png" alt="image-20221221155646489"></p>
<p>​    Strategies and the objectives of the transfer learning approaches from the data perspective</p>
<h2 id="4-Data-based-Interpretation"><a href="#4-Data-based-Interpretation" class="headerlink" title="4. Data-based Interpretation"></a>4. Data-based Interpretation</h2><h3 id="4-1-Instance-Weighting-Strategy"><a href="#4-1-Instance-Weighting-Strategy" class="headerlink" title="4.1 Instance Weighting Strategy"></a>4.1 Instance Weighting Strategy</h3><p>比如 由于源域的样本分布和目标域的样本分布不同，所以直接用源域学习到的模型/函数来预测目标域是不可行的，所以必须要对源域的样本设置权重。</p>
<h3 id="4-2-Feature-Transformation-Strategy"><a href="#4-2-Feature-Transformation-Strategy" class="headerlink" title="4.2 Feature Transformation Strategy"></a>4.2 Feature Transformation Strategy</h3><ul>
<li>期望的目标<ul>
<li>source 和 target 上的边缘分布和条件分布尽可能一致</li>
<li>保留数据中隐含的性质或者结构</li>
<li>寻找特征之间的相关性</li>
</ul>
</li>
</ul>
<p>样本 + 特征 == 数据</p>
<p>特征变换操作的三种类型</p>
<ul>
<li>特征增强</li>
<li>特征缩减<ul>
<li>特征映射</li>
<li>特征聚集</li>
<li>特征选择</li>
<li>特征编码</li>
</ul>
</li>
<li>特征对齐</li>
</ul>
<h4 id="4-2-1-Distribution-Difference-Metric"><a href="#4-2-1-Distribution-Difference-Metric" class="headerlink" title="4.2.1 Distribution Difference Metric"></a>4.2.1 Distribution Difference Metric</h4><p>因为特征转换的目标之一其实就是减少源域样本和目标域样本之间的分布差异</p>
<p>所以有效准确的度量分布差异变得很重要</p>
<ul>
<li>最大平均差异MMD：迁移学习中最常用的分布差异度量指标<img src="https://gitee.com/fjkf/images/raw/master/imgs/202212211953837.png" alt="image-20221221195315813"></li>
<li>当然还有很多其他的度量方式<ul>
<li><strong>Maximum Mean Discrepancy</strong></li>
<li><strong>Kullback-Leibler Divergence</strong></li>
<li><strong>Jensen-Shannon Divergence</strong></li>
<li><strong>Bregman Divergence</strong></li>
<li><strong>Hilbert-Schmidt Independence Criterion</strong></li>
<li><strong>Wasserstein distiance</strong></li>
<li><strong>Central moment discrepancy</strong></li>
</ul>
</li>
</ul>
<h4 id="4-2-2-Feature-Augmentation"><a href="#4-2-2-Feature-Augmentation" class="headerlink" title="4.2.2 Feature Augmentation"></a>4.2.2 Feature Augmentation</h4><blockquote>
<p>针对source domain 和 target domain上的特征不同，可以对原来的feature进行一定的增广，从而获得三类不同的特征 (1) 通用特征 (2) source domain特有特征 (3) target domain 特有特征。</p>
</blockquote>
<ul>
<li><p>对于同质的迁移学习任务，可以做简单的 对特征进行复制，得到如下的特征映射，FAM(Feature Augmentation Method)的新特征表示如下: $\Phi_S(\mathbf{x}_i^S)=\langle\mathbf{x}_i^S,\mathbf{x}_i^S,\mathbf{0}\rangle,\Phi_T(\mathbf{x}_j^T)=\langle\mathbf{x}_j^T,\mathbf{0},\mathbf{x}_j^T\rangle$ ，</p>
<p>有了这样的新的特征，就可以在source 和 target上同时的进行训练。</p>
</li>
</ul>
<p>​        其中ΦS和ΦT分别表示从源域和目标域到新特征空间的映射</p>
<ul>
<li>对于异质的迁移学习任务，对于通用特征，就需要转换到相同的维度，则需要额外的学习一个映射 W^S,W^T, 所以增广的目标如下：HFA（异构特征增强）:$\Phi<em>{S}(\mathbf{x}_i^{S})=\langle W^{S}\mathbf{x}_i^{S},\mathbf{x}_i^{S},\mathbf{0}^T\rangle,\Phi</em>{T}(\mathbf{x}_j^{T})=\langle W^{T}\mathbf{x}_j^{T},\mathbf{0}^{S},\mathbf{x}_j^{T}\rangle$ </li>
</ul>
<h4 id="4-2-3-Feature-Mapping"><a href="#4-2-3-Feature-Mapping" class="headerlink" title="4.2.3 Feature Mapping"></a>4.2.3 Feature Mapping</h4><p>用feature mapping的方法来从source domian 和 target domain来映射得到一些分布相近的特征。然而常见的像PCA这样的算法，其主要关注于的是方差，而不是分布的相似性，所以feature mapping的目标是学习到一个mapping的函数$\boldsymbol{\Phi}$ ，优化如下的目标 $ \operatorname*{min}_{\Phi}\big(\mathrm{DIST}(X^{S},X^{T};\Phi)+\lambda\Omega(\Phi)\big)/\big(\mathrm{VAR}(X^{S}\cup X^{T};\Phi)\big)$ </p>
<p>很明显，想要获得一个representation，使得source domain 和 target domain 上的分布尽可能相近，并且数据的方差要比较大，方便后续区分（也避免0映射）</p>
<p><strong>保有数据的结构特征</strong> 如果需要对数据的结构特征进行学习及限制，可以增加得到如下的限制条件：$\min\limits_{\Phi}\mu DIST(X^S,X^T;\Phi)+\lambda_1\Omega^{GEO}(\Phi)+\lambda_2\Omega(\Phi)+(1-\mu)DIST(Y^S|X^S,Y^T|X^T|X^T|X^T|X^T|$ </p>
<p>s.t. $\Phi(X)^{\mathrm{T}}H\Phi(X)=I,\mathrm{with}H=I-(\mathbf{1}/n)\in\mathbb{R}^{n\times n}$ ，</p>
<p>以上的优化目标中，除了边缘分布的相似性，同样也考虑了条件分布的相似性</p>
<h4 id="Feature-selection"><a href="#Feature-selection" class="headerlink" title="Feature selection"></a>Feature selection</h4><p>特征选择，目标是在source 和 target domain中选择出，作用相同的特征出来，这些特征可以作为知识迁移的桥梁，做法如下</p>
<ul>
<li><p>首先对特征进行选择， 选择出在source和target domain上表现相同的特征出来（pivot feature)</p>
</li>
<li><p>用这些相似特征学一个低维的新特征</p>
</li>
<li><ul>
<li>比如用其它的特征，来预测某一个pivot feature的值，得到模型 fi ，其参数为 θi</li>
<li>将这些额外获得的 θi 组合成 一个新的矩阵 W ，并且对 W 进行奇异值分解，获得前 k 阶主成分 W′</li>
<li>从而获得新的低维特征 W′X</li>
</ul>
</li>
<li><p>将低维的新特征作为增强的特征，叠加在原来的特征上，再进行新的模型训练。</p>
</li>
</ul>
<h4 id="Feature-Encoding"><a href="#Feature-Encoding" class="headerlink" title="Feature Encoding"></a>Feature Encoding</h4><p>这个方法就是利用autoencoder来学习一个新的特征编码，然后所有样本的特征转换到autoencoder学到的特征编码这个space上去，再在这个新的space上学习模型。</p>
<p>具体的步骤就是</p>
<ul>
<li>先用autoencoder 对source domain 和 target domain 进行训练（stack多个降噪自编码器）</li>
<li>然后将每层中间的latent variable当作representation叠加在一起</li>
<li>然后在有标签数据上进行监督学习</li>
</ul>
<h4 id="Feature-Alignment"><a href="#Feature-Alignment" class="headerlink" title="Feature Alignment"></a>Feature Alignment</h4><p>在前面提到的一些方法中，往往都是利用一些显性的特征（比如feature selection 或着 feature augmentation）但是实际上还有许多的隐性的特征是可以利用的，比如<strong>子空间特征、谱特征、统计特征</strong>。对于这些隐性特征，我们可以分别在source domain 和 target domain计算这些特征，并且研究这些隐性特征如何可以在两个domain上进行对齐，以<strong>子空间特征为例</strong></p>
<ul>
<li><p>为source domain 和 target domain 生成各自的子空间，子空间可以用一组相互正交的基来表示 $M_S, M_T$ （具体的子空间求解可以由PCA来求，用最大几个特征值对应的特征向量来表示）</p>
</li>
<li><p>然后利用 $M_S, M_T$ 来学习如何将两个空间进行对齐，如学习一个转移矩阵 W :</p>
</li>
<li><ul>
<li>$W=arg\operatorname*{min}_W||M_SW-M_T||_F^2$ </li>
<li>其中 ||∗||F  表示的是 <strong>Frobenius Norm</strong></li>
</ul>
</li>
<li><p>最后用转移矩阵对齐两个空间的特征，然后一起学习</p>
</li>
</ul>
<h2 id="5-Model-based-Interpretation"><a href="#5-Model-based-Interpretation" class="headerlink" title="5. Model-based Interpretation"></a>5. Model-based Interpretation</h2><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212212115000.png" alt="image-20221221211513915"></p>
<h3 id="1-Model-Control-Strategy"><a href="#1-Model-Control-Strategy" class="headerlink" title="1. Model Control Strategy"></a>1. Model Control Strategy</h3><p>在模型的层面上，一类直接并且简单的方法是直接在迁移的过程中，增加<strong>一些正则函数</strong>来帮助迁移知识，目标函数可以统一的表示为如下的形式</p>
<p>​                     $\underset{f^T}{\min}\mathcal{L}^{T,L}(f^T)+\lambda_1\Omega^D(f^T)+\lambda_2\Omega(f^T)$ </p>
<p>其中第一项表示的是优化的目标函数，比如优化均方误差，第二项的则用来统一代表不同的正则化函数，第三项则是用来限制模型的复杂程度的。</p>
<h4 id="DAM"><a href="#DAM" class="headerlink" title="DAM"></a>DAM</h4><p>领域自适应机器（DAM）的通用框架，<strong>该框架设计用于多源转移学习</strong>，DAM的目标是借助于分别在多个源域上训练的一些预先获得的基础分类器，为目标域构造一个鲁棒的分类器</p>
<h4 id="Consensus-Regularizer-（CRF）"><a href="#Consensus-Regularizer-（CRF）" class="headerlink" title="Consensus Regularizer （CRF）"></a>Consensus Regularizer （CRF）</h4><p>该方法是针对于target domain上没有标签数据的情形，并且拥有多个 source domain，在这些source domain上会分别建立$m_s$个分类器，记作$f_k^S(k=1,\ldots,m^S)$ ,此时需要的解决的问题就是如何学习这些分类器，使得他们能够在target domain上有比较好的表现。</p>
<p>每个分类器的优化目标为:</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212261051321.png" alt="image-20221226105056212"></p>
<p>其中$S(x)=-x\textit{log}x$ </p>
<p>在上面优化目标中第一项表示优化source domain的分类误差，而最后一项则是在优化交叉熵。这样做的目的在于，提高不同的分类器在target domain上的共识，并且降低在target domain上预测的不确定性。</p>
<h4 id="Domain-dependent-Regularizer"><a href="#Domain-dependent-Regularizer" class="headerlink" title="Domain-dependent Regularizer"></a>Domain-dependent Regularizer</h4><p>该方法是针对于target domain上包含有标签数据 和 无标签数据的情形，并且拥有多个source domain，直接来看下面的优化目标</p>
<p>$\begin{aligned}\min<em>{f^T}\sum</em>{j=1}^{n^{T,L}}\left(f^{T}(\mathbf{x}<em>{j}^{T,L})-y</em>{j}^{T,L}\right)^2+\lambda<em>{2}\Omega(f^{T})\ +\lambda</em>{1}\sum<em>{k=1}^{m^{S}}\beta</em>{k}\sum<em>{i=1}^{n^{T,U}}\left(f^{T}(\mathbf{x}</em>{i}^{T,U})-f<em>{k}^{S}(\mathbf{x}</em>{i}^{T,U})\right)^2\end{aligned}$ </p>
<p>可以看出，第一项是在优化target domain 上的均方误差，而最后一项是在无标签数据上进行优化，希望在无标签数据上，每个source domain上的预测结果与target domain上的结果尽可能相近。并且用一个参数 $\beta_k$ 对不同的source domain进行加权，该参数表示的是target domain 和 source domain之间的相关性，可以用 <em>MMD</em> 指标来计算.</p>
<h3 id="2-Parameter-Control-Strategy"><a href="#2-Parameter-Control-Strategy" class="headerlink" title="2. Parameter Control Strategy"></a>2. Parameter Control Strategy</h3><p>参数控制的策略是在模型的参数的层面进行迁移学习。一般来说，模型的参数实际上就是反映着模型学习到的知识，比如图片中的物体属性，这些属性的一些先验分布的参数等。</p>
<h4 id="1-Parameter-Sharing"><a href="#1-Parameter-Sharing" class="headerlink" title="1. Parameter Sharing"></a>1. Parameter Sharing</h4><h5 id="MTrick"><a href="#MTrick" class="headerlink" title="MTrick"></a>MTrick</h5><p>(Matrix Tri-Factorization Based Classification Framework)</p>
<p>参数共享的方法是很容易想到的方法，在source domain上学习到的模型在参数，在target domain上微调模型时，只微调最后几层，或者是直接用前几层的特征。</p>
<p>另外还可以想想还有没有什么其它的参数信息是在不同的domain上是比较具有共性并且可以共享的一些参数——矩阵分解。比如说在文本领域，我们都知道文本是由单词组成，如果说我们可以获得单词背后的内在含义作为特征，那么这些特征应当是比单词自身更有效的。具体的做法是构建三个矩阵</p>
<ul>
<li><em>Document-to-cluster Matrix</em> Q</li>
<li><em>Connection Matrix</em> R</li>
<li><em>Cluster-to-word Matrix</em> W</li>
</ul>
<p>原本的矩阵为 X，所以优化的目标函数为：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212291356017.png" alt="image-20221229135608983"></p>
<p>值得注意的是，R 是在source domain 和 target domain 上公用的知识。最后一项实际是用了标签信息来优化， Q^S 表示的是每个样本的分类。</p>
<h5 id="TriTL"><a href="#TriTL" class="headerlink" title="TriTL"></a>TriTL</h5><p>全称：Triplex Transfer Learning</p>
<p>另外上述的关系还可以进一步分解（<em>TriTL</em>），分解成更细致的特征，从而得到如下的优化方式：</p>
<p>$\operatorname*{min}<em>{Q,R,W}\sum\limits</em>{k=1}^{m^S+m^T}||X_k-Q_k\begin{bmatrix}R^{\mathrm{DI}}&amp;R^{\mathrm{TD}}&amp;R^{\mathrm{TD}}_k\end{bmatrix}\begin{bmatrix}W^{\mathrm{DI}}\ W_k^{\mathrm{TD}}\ W_k^{\mathrm{ND}}\end{bmatrix}||^2$ </p>
<p>s.t. Normalization Constraints</p>
<blockquote>
<p>一些内容的新名词：</p>
<p>基于神经网络的参数共享</p>
<p>基于矩阵分解的参数共享</p>
<p>迁移学习可用于图像分类</p>
<p>MTrick</p>
<p>基于PLSA的转移学习  利用构建贝叶斯网络的概念</p>
<p>矩阵三因子分解：将  文档到单词的矩阵 分解为 <strong>文档到簇的矩阵</strong> ， <strong>连接矩阵</strong>，<strong>簇到单词的矩阵</strong></p>
<p>  QRW： R实际上是共享参数</p>
<p> tri-factorize the source document-to-word matrix  1</p>
<p>decomposes the target document-to-word matrix   2</p>
<p>包含源域标签信息  3</p>
</blockquote>
<h4 id="2-Parameter-Restriction"><a href="#2-Parameter-Restriction" class="headerlink" title="2. Parameter Restriction"></a>2. Parameter Restriction</h4><p>上面的参数共享，让source domain 和 target domain 公用一些参数，而在参数限制的方法中，则是要求两个domain上的某些参数尽可能相似。</p>
<p>$\min\limits<em>f\dfrac{1}{2}\left|\left|\theta-\beta\tilde{\theta}\right|\right|^2+\dfrac{\lambda}{2}\sum\limits</em>{j=1}^{n^{T,L}}\eta_j\left(f\left(\mathbf{x}_j^{T,L}\right)-y_j^{T,L}\right)^2$ </p>
<p>如果有多个训练好的分类器，也可以拓展到如下的优化目标：</p>
<p>$\min\limits<em>f\dfrac{1}{2}\left|\left|\boldsymbol{\theta}-\sum\limits</em>{i=1}^k\beta<em>i\boldsymbol{\theta}_i\right|\right|^2+\dfrac{\lambda}{2}\sum\limits</em>{j=1}^{n^{T,L}}\eta_j\left(f(\mathbf{x}_j^{T,L})-y_j^{T,L}\right)^2$ </p>
<blockquote>
<p>单一模型知识迁移  SMKL （Single-Model KnowledgeTransfer ）</p>
<p>category learning 类别学习</p>
<p>类别学习问题的一个解决方案：SMKL（SIngle-Model Knowledge Transfer）基于 LS-SVM</p>
<p>LS-SVM的优点是将不等式约束转换为等式约束，高计算效率，其优化相当于求解线性方程组而不是二次规划问题</p>
</blockquote>
<h3 id="3-Model-Ensemble-Strategy"><a href="#3-Model-Ensemble-Strategy" class="headerlink" title="3. Model Ensemble Strategy"></a>3. Model Ensemble Strategy</h3><p>模型融合的方法，也是一个比较容易想到的策略，融合在多个source domains上的分类器结果，期望在target domain上效果有所改进。</p>
<p>将多个弱分类器结合来最终预测</p>
<p>模型集成  —》 先去了解一下基础 ，什么是AdaBoost，看西瓜书去了，稍后看完再来继续啃这个文献</p>
<blockquote>
<p>LWE  (Locally Weighted Ensemble) 基于图的方法来更新/生成权重</p>
</blockquote>
<h4 id="TaskTrAdaBoost"><a href="#TaskTrAdaBoost" class="headerlink" title="TaskTrAdaBoost"></a>TaskTrAdaBoost</h4><p>这个方法与 <em>AdaBoost</em> 的思想类似，每一轮次要得到一个新的弱分类器。但是<em>TaskTrAdaBoost</em>首先是在source domain上训练多个弱分类器，形成一个分类器池，然后再在target domain上衡量这些弱分类器的表现，每次挑一个表现最好的分类器出来，然后再给target domain上的样本按照分类误差进行赋权。</p>
<h4 id="Locally-Weighted-Ensemble"><a href="#Locally-Weighted-Ensemble" class="headerlink" title="Locally Weighted Ensemble"></a>Locally Weighted Ensemble</h4><p>前一个提到的方法中，是对每个分类器给予一个权重，相当于该分类器对所有的样本的权重是一致的。这个方法的思路是每个分类器对于不同的样本的权重也应该不同。方法是对于一个分类器在target domain上对每个样本进行分类，如果判断属于同一个类别的，则给这两个样本连一条边，最终形成一个图 $G_S^T$ ，另外用一个聚类算法得到一个图 $G^T$。然后对于不同样本的权重，则根据两个图上样本的相关性来计算。</p>
<h4 id="Ensmble-Framework-of-Anchor-Adapters-ENCHOR"><a href="#Ensmble-Framework-of-Anchor-Adapters-ENCHOR" class="headerlink" title="Ensmble Framework of Anchor Adapters (ENCHOR)"></a>Ensmble Framework of Anchor Adapters (ENCHOR)</h4><p>前面方法关注于如何加权，这个方法关注与如何构建弱分类器。</p>
<p>方法是会用一些锚点方法为样本产生不同的特征，然后对样本上的不同特征进行训练模型，然后融合在一起。</p>
<h3 id="4-Deep-Learning-Techniuqe"><a href="#4-Deep-Learning-Techniuqe" class="headerlink" title="4. Deep Learning Techniuqe"></a>4. Deep Learning Techniuqe</h3><p>深度学习方法</p>
<h4 id="1-Traditional-Deep-Learning"><a href="#1-Traditional-Deep-Learning" class="headerlink" title="1. Traditional Deep Learning"></a>1. Traditional Deep Learning</h4><h5 id="TLDA（Transfer-Learning-with-Deep-Autoencoders）"><a href="#TLDA（Transfer-Learning-with-Deep-Autoencoders）" class="headerlink" title="TLDA（Transfer Learning with Deep Autoencoders）"></a>TLDA（Transfer Learning with Deep Autoencoders）</h5><p>TLDA分别对源域和目标域采用两个自动编码器。这两个自动编码器共享相同的参数。编码器和解码器都具有两个具有激活功能的层。两个自动编码器的示意图如下所示：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212281258486.png" alt="image-20221228125801364"></p>
<p>目标函数如下 $\begin{array}{c}\min\mathcal{L}<em>{\mathrm{REC}}(X,\tilde{X})+\lambda_1\mathrm{KL}(Q^S||Q^T)+\lambda_2\Omega(W,b,\hat{W},\hat{b})\ +\lambda_3\mathcal{L}</em>{\mathrm{REG}}(R^S,Y^S),\end{array}$ </p>
<p>该方法在source domain和target domain上的特征之间用 <em>KL</em> 距离进行优化，并且对source domain上的特征 RS ，优化一个预测损失。</p>
<p>使用梯度下降法来训练降低误差</p>
<p>预测有两种方式</p>
<ul>
<li>直接使用编码器的输出来进行预测</li>
<li>将编码器的第一层输出产生的特征表示在标记实例上训练目标分类器</li>
</ul>
<h5 id="DAN（Deep-Adaptation-Networks-）"><a href="#DAN（Deep-Adaptation-Networks-）" class="headerlink" title="DAN（Deep Adaptation Networks ）"></a>DAN（Deep Adaptation Networks ）</h5><p>这个方法是在提取特征的过程中增加一些限制条件，如下：</p>
<p>它是一种使用了多层自适应层和多核技术的一个深度学习网络，基于AlexNet</p>
<p>其架构如下：<img src="https://gitee.com/fjkf/images/raw/master/imgs/202212281305714.png" alt="image-20221228130524675"></p>
<p>用五层卷积层提取特征，用三层的全连接层来得到源域和目标域的函数</p>
<p>目标函数如下：$\underset{\Theta}{\min}\max\limits<em>{\kappa}\sum\limits</em>{i=1}^{n^L}\mathcal{L}\left(f(\mathbf{x}<em>i^L),y_i^L\right)+\lambda\sum\limits</em>{l=6}^{8}\text{MK-MMD}(R_l^S,R_l^T;\kappa),$</p>
<h5 id="Multiple-Feature-Spaces-Adaption-Network-MFSAN"><a href="#Multiple-Feature-Spaces-Adaption-Network-MFSAN" class="headerlink" title="Multiple Feature Spaces Adaption Network(MFSAN)"></a>Multiple Feature Spaces Adaption Network(MFSAN)</h5><p>该方法会有设计通用的特征提取器，以及domain相关的提取器，具体的流程如下：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212281317207.png" alt="image-20221228131725162"></p>
<p>目标函数：$\begin{aligned}\min<em>{\Theta}\sum</em>{i=1}^{m^S}\mathcal{L}(\hat{Y}<em>i^S,Y_i^S)+\lambda_1\sum</em>{i=1}^{m^S}\text{MMD}(R<em>i^S,R_i^T) &amp;+\lambda_2\sum</em>{i\neq j}^{m^S}\left|\hat{Y}_i^T-\hat{Y}_j^T\right|,\end{aligned}$</p>
<p>整体的思想还是提取到的特征分布要尽可能的相近，并且不同的domain-specific上的模型，在target domain上要表现尽可能相近。</p>
<h4 id="2-Adversarial-Deep-Learning-对抗学习的方法"><a href="#2-Adversarial-Deep-Learning-对抗学习的方法" class="headerlink" title="2. Adversarial Deep Learning 对抗学习的方法"></a>2. Adversarial Deep Learning 对抗学习的方法</h4><p>利用对抗学习的方法构造成min max的问题来优化，并且有source domain的数据，能很容易构造对抗样本，流程框图如下：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212281324207.png" alt="image-20221228132442170"></p>
<p>对于source domain 和 target domain 都要提取特征来进行预测，另外有一个 <em>domain classifier</em> ，这个用来充当GAN中的D的角色，要求分辨不出数据是来自于哪个domain，从而在两个domain上能够提取到足够相近的特征。</p>
<h2 id="7-Experiment"><a href="#7-Experiment" class="headerlink" title="7.  Experiment"></a>7.  Experiment</h2><h3 id="1-Dataset-and-Preprocessing"><a href="#1-Dataset-and-Preprocessing" class="headerlink" title="1. Dataset and Preprocessing"></a>1. Dataset and Preprocessing</h3><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212291312362.png" alt="image-20221229131247173"></p>
<h3 id="2-Experiment-Setting"><a href="#2-Experiment-Setting" class="headerlink" title="2. Experiment Setting"></a>2. Experiment Setting</h3><p>an SVM with a linear kernel   作为baseline</p>
<p>源域的实例全已被标记</p>
<p>对于执行的算法（TrAdaBoost除外），目标域实例未标记</p>
<p>每个算法执行三次，取平均作为实验结果</p>
<h2 id="Conclusion-and-Future-Direction"><a href="#Conclusion-and-Future-Direction" class="headerlink" title="Conclusion and Future Direction"></a>Conclusion and Future Direction</h2><ol>
<li>可以进一步探索迁移学习技术，并将其应用于更广泛的应用</li>
<li>如何衡量跨域的可迁移性并避免负迁移也是一个重要问题。</li>
<li>迁移学习的可解释性也需要进一步研究</li>
<li>进一步进行理论研究，为迁移学习的有效性和适用性提供理论支持</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://wdpname.github.io">dpWu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wdpname.github.io/2022/12/19/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/">https://wdpname.github.io/2022/12/19/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wdpname.github.io" target="_blank">平博社</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">迁移学习</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/22/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">《机器学习实战》</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/19/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">名词解释</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/01/11/Domain-Prompt-Learning-for-Efficiently-Adapting-CLIP-to-Unseen-Domains/" title="Domain Prompt Learning for Efficiently Adapting CLIP to Unseen Domains"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-11</div><div class="title">Domain Prompt Learning for Efficiently Adapting CLIP to Unseen Domains</div></div></a></div><div><a href="/2022/12/19/A-perspective-survey-on-deep-transfer-learning-for-fault-diagnosis-in-industrial-scenarios-Theories-applications-and-challenges/" title="A perspective survey on deep transfer learning for fault diagnosis in industrial scenarios: Theories, applications and challenges"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-19</div><div class="title">A perspective survey on deep transfer learning for fault diagnosis in industrial scenarios: Theories, applications and challenges</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dpWu</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">121</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdpname" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://1939317922@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95"><span class="toc-number">1.</span> <span class="toc-text">迁移学习算法研究进展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E6%BA%90%E9%A2%86%E5%9F%9F%E5%92%8C%E7%9B%AE%E6%A0%87%E9%A2%86%E5%9F%9F%E6%A0%B7%E6%9C%AC%E6%98%AF%E5%90%A6%E6%A0%87%E6%B3%A8%E4%BB%A5%E5%8F%8A%E4%BB%BB%E5%8A%A1%E6%98%AF%E5%90%A6%E7%9B%B8%E5%90%8C%E5%88%92%E5%88%86"><span class="toc-number">1.1.</span> <span class="toc-text">针对源领域和目标领域样本是否标注以及任务是否相同划分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E6%BA%90%E9%A2%86%E5%9F%9F%E5%92%8C%E7%9B%AE%E6%A0%87%E9%A2%86%E5%9F%9F%E4%B8%AD%E6%98%AF%E5%90%A6%E6%9C%89%E6%A0%87%E7%AD%BE%E6%A0%B7%E6%9C%AC%E5%88%92%E5%88%86"><span class="toc-number">1.1.1.</span> <span class="toc-text">根据源领域和目标领域中是否有标签样本划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E6%BA%90%E9%A2%86%E5%9F%9F%E4%B8%AD%E6%98%AF%E5%90%A6%E6%9C%89%E6%A0%87%E7%AD%BE%E6%A0%B7%E6%9C%AC"><span class="toc-number">1.1.2.</span> <span class="toc-text">根据源领域中是否有标签样本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E5%92%8C%E6%B5%8B%E8%AF%95%E6%A0%B7%E6%9C%AC%E6%98%AF%E5%90%A6%E6%9D%A5%E8%87%AA%E4%BA%8E%E5%90%8C%E4%B8%80%E4%B8%AA%E9%A2%86%E5%9F%9F"><span class="toc-number">1.1.3.</span> <span class="toc-text">根据训练样本和测试样本是否来自于同一个领域</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%89%E7%85%A7%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E9%87%87%E7%94%A8%E7%9A%84%E6%8A%80%E6%9C%AF%E5%88%92%E5%88%86"><span class="toc-number">1.2.</span> <span class="toc-text">按照迁移学习方法采用的技术划分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.1.</span> <span class="toc-text">半监督学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%90%84%E7%A7%8D%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">2.</span> <span class="toc-text">传统学习和各种迁移学习的关系</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#a-comprehensive-survey-on-transfer-learning"><span class="toc-number">3.</span> <span class="toc-text">a comprehensive survey on transfer learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E6%83%B3%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9C%BA%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">理想的机器学习场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E9%A2%86%E5%9F%9F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%9A"><span class="toc-number">3.2.</span> <span class="toc-text">根据领域之间的差异：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-number">3.3.</span> <span class="toc-text">2. related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-1"><span class="toc-number">3.3.1.</span> <span class="toc-text">半监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.3.2.</span> <span class="toc-text">多视图学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.3.3.</span> <span class="toc-text">多任务学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Overview"><span class="toc-number">3.4.</span> <span class="toc-text">3.Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Categorization-of-Transfer-Learning"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.3. Categorization of Transfer Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Data-based-Interpretation"><span class="toc-number">3.5.</span> <span class="toc-text">4. Data-based Interpretation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Instance-Weighting-Strategy"><span class="toc-number">3.5.1.</span> <span class="toc-text">4.1 Instance Weighting Strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Feature-Transformation-Strategy"><span class="toc-number">3.5.2.</span> <span class="toc-text">4.2 Feature Transformation Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Distribution-Difference-Metric"><span class="toc-number">3.5.2.1.</span> <span class="toc-text">4.2.1 Distribution Difference Metric</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Feature-Augmentation"><span class="toc-number">3.5.2.2.</span> <span class="toc-text">4.2.2 Feature Augmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-Feature-Mapping"><span class="toc-number">3.5.2.3.</span> <span class="toc-text">4.2.3 Feature Mapping</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-selection"><span class="toc-number">3.5.2.4.</span> <span class="toc-text">Feature selection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-Encoding"><span class="toc-number">3.5.2.5.</span> <span class="toc-text">Feature Encoding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-Alignment"><span class="toc-number">3.5.2.6.</span> <span class="toc-text">Feature Alignment</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Model-based-Interpretation"><span class="toc-number">3.6.</span> <span class="toc-text">5. Model-based Interpretation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Model-Control-Strategy"><span class="toc-number">3.6.1.</span> <span class="toc-text">1. Model Control Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DAM"><span class="toc-number">3.6.1.1.</span> <span class="toc-text">DAM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Consensus-Regularizer-%EF%BC%88CRF%EF%BC%89"><span class="toc-number">3.6.1.2.</span> <span class="toc-text">Consensus Regularizer （CRF）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Domain-dependent-Regularizer"><span class="toc-number">3.6.1.3.</span> <span class="toc-text">Domain-dependent Regularizer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Parameter-Control-Strategy"><span class="toc-number">3.6.2.</span> <span class="toc-text">2. Parameter Control Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Parameter-Sharing"><span class="toc-number">3.6.2.1.</span> <span class="toc-text">1. Parameter Sharing</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MTrick"><span class="toc-number">3.6.2.1.1.</span> <span class="toc-text">MTrick</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#TriTL"><span class="toc-number">3.6.2.1.2.</span> <span class="toc-text">TriTL</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Parameter-Restriction"><span class="toc-number">3.6.2.2.</span> <span class="toc-text">2. Parameter Restriction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Model-Ensemble-Strategy"><span class="toc-number">3.6.3.</span> <span class="toc-text">3. Model Ensemble Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TaskTrAdaBoost"><span class="toc-number">3.6.3.1.</span> <span class="toc-text">TaskTrAdaBoost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Locally-Weighted-Ensemble"><span class="toc-number">3.6.3.2.</span> <span class="toc-text">Locally Weighted Ensemble</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Ensmble-Framework-of-Anchor-Adapters-ENCHOR"><span class="toc-number">3.6.3.3.</span> <span class="toc-text">Ensmble Framework of Anchor Adapters (ENCHOR)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Deep-Learning-Techniuqe"><span class="toc-number">3.6.4.</span> <span class="toc-text">4. Deep Learning Techniuqe</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Traditional-Deep-Learning"><span class="toc-number">3.6.4.1.</span> <span class="toc-text">1. Traditional Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#TLDA%EF%BC%88Transfer-Learning-with-Deep-Autoencoders%EF%BC%89"><span class="toc-number">3.6.4.1.1.</span> <span class="toc-text">TLDA（Transfer Learning with Deep Autoencoders）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#DAN%EF%BC%88Deep-Adaptation-Networks-%EF%BC%89"><span class="toc-number">3.6.4.1.2.</span> <span class="toc-text">DAN（Deep Adaptation Networks ）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Multiple-Feature-Spaces-Adaption-Network-MFSAN"><span class="toc-number">3.6.4.1.3.</span> <span class="toc-text">Multiple Feature Spaces Adaption Network(MFSAN)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Adversarial-Deep-Learning-%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">3.6.4.2.</span> <span class="toc-text">2. Adversarial Deep Learning 对抗学习的方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Experiment"><span class="toc-number">3.7.</span> <span class="toc-text">7.  Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Dataset-and-Preprocessing"><span class="toc-number">3.7.1.</span> <span class="toc-text">1. Dataset and Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Experiment-Setting"><span class="toc-number">3.7.2.</span> <span class="toc-text">2. Experiment Setting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion-and-Future-Direction"><span class="toc-number">3.8.</span> <span class="toc-text">Conclusion and Future Direction</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%A7%AF%E7%B4%AF/" title="深度学习代码积累"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习代码积累"/></a><div class="content"><a class="title" href="/2023/04/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%A7%AF%E7%B4%AF/" title="深度学习代码积累">深度学习代码积累</a><time datetime="2023-04-25T04:57:30.000Z" title="发表于 2023-04-25 12:57:30">2023-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/25/%E5%AF%B9%E6%96%87%E4%BB%B6%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%80%9D%E8%80%83/" title="对文件编码格式的思考"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="对文件编码格式的思考"/></a><div class="content"><a class="title" href="/2023/04/25/%E5%AF%B9%E6%96%87%E4%BB%B6%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%80%9D%E8%80%83/" title="对文件编码格式的思考">对文件编码格式的思考</a><time datetime="2023-04-25T02:40:54.000Z" title="发表于 2023-04-25 10:40:54">2023-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/21/%E5%AF%B9%E4%BA%8E%E6%98%AF%E5%90%A6%E4%BD%BF%E7%94%A8%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E9%AA%8C/" title="对于是否使用标准化的数据的一些实验"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="对于是否使用标准化的数据的一些实验"/></a><div class="content"><a class="title" href="/2023/04/21/%E5%AF%B9%E4%BA%8E%E6%98%AF%E5%90%A6%E4%BD%BF%E7%94%A8%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E9%AA%8C/" title="对于是否使用标准化的数据的一些实验">对于是否使用标准化的数据的一些实验</a><time datetime="2023-04-21T13:11:47.000Z" title="发表于 2023-04-21 21:11:47">2023-04-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/17/Malware-Image-Classification-using-Machine-Learning-with-Local-Binary-Pattern/" title="Malware Image Classification using Machine Learning with Local Binary Pattern"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Malware Image Classification using Machine Learning with Local Binary Pattern"/></a><div class="content"><a class="title" href="/2023/04/17/Malware-Image-Classification-using-Machine-Learning-with-Local-Binary-Pattern/" title="Malware Image Classification using Machine Learning with Local Binary Pattern">Malware Image Classification using Machine Learning with Local Binary Pattern</a><time datetime="2023-04-17T01:53:45.000Z" title="发表于 2023-04-17 09:53:45">2023-04-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2/" title="动手学深度学习v2"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动手学深度学习v2"/></a><div class="content"><a class="title" href="/2023/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2/" title="动手学深度学习v2">动手学深度学习v2</a><time datetime="2023-04-16T03:11:05.000Z" title="发表于 2023-04-16 11:11:05">2023-04-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By dpWu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>