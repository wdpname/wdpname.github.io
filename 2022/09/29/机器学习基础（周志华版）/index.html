<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>机器学习基础（周志华版） | 平博社</title><meta name="keywords" content="ML"><meta name="author" content="dpWu"><meta name="copyright" content="dpWu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="机器学习分类 分类一  监督学习：分类，回归 无监督学习（对抗网络GAN）：聚类 半监督学习 增强学习（alpha go围棋）   分类二  批量学习 在线学习   分类三  参数学习 非参数学习    主要符号表 第一章 绪论一，引言 模型：泛指从数据中学得的结果。（有文献用“模型”指全局性结果（例如一颗决策树），而用“模式”指局部性结果（例如一条规则）） 学习算法：从数据中产生“模型”的算法">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础（周志华版）">
<meta property="og:url" content="https://wdpname.github.io/2022/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E5%91%A8%E5%BF%97%E5%8D%8E%E7%89%88%EF%BC%89/index.html">
<meta property="og:site_name" content="平博社">
<meta property="og:description" content="机器学习分类 分类一  监督学习：分类，回归 无监督学习（对抗网络GAN）：聚类 半监督学习 增强学习（alpha go围棋）   分类二  批量学习 在线学习   分类三  参数学习 非参数学习    主要符号表 第一章 绪论一，引言 模型：泛指从数据中学得的结果。（有文献用“模型”指全局性结果（例如一颗决策树），而用“模式”指局部性结果（例如一条规则）） 学习算法：从数据中产生“模型”的算法">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-09-29T11:18:46.000Z">
<meta property="article:modified_time" content="2023-02-27T01:40:55.226Z">
<meta property="article:author" content="dpWu">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/assets/avatar.jpeg"><link rel="canonical" href="https://wdpname.github.io/2022/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E5%91%A8%E5%BF%97%E5%8D%8E%E7%89%88%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习基础（周志华版）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-27 09:40:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">160</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">平博社</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">机器学习基础（周志华版）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-29T11:18:46.000Z" title="发表于 2022-09-29 19:18:46">2022-09-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-27T01:40:55.226Z" title="更新于 2023-02-27 09:40:55">2023-02-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习基础（周志华版）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h1><ol>
<li><p>分类一</p>
<ul>
<li>监督学习：分类，回归</li>
<li>无监督学习（对抗网络GAN）：聚类</li>
<li>半监督学习</li>
<li>增强学习（alpha go围棋）</li>
</ul>
</li>
<li><p>分类二</p>
<ul>
<li>批量学习</li>
<li>在线学习</li>
</ul>
</li>
<li><p>分类三</p>
<ul>
<li>参数学习</li>
<li>非参数学习</li>
</ul>
</li>
</ol>
<h1 id="主要符号表"><a href="#主要符号表" class="headerlink" title="主要符号表"></a>主要符号表</h1><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212112108297.png" alt="image-20221211210820149"></p>
<h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="一，引言"><a href="#一，引言" class="headerlink" title="一，引言"></a>一，引言</h2><ul>
<li>模型：泛指从数据中学得的结果。（有文献用“模型”指全局性结果（例如一颗决策树），而用“模式”指局部性结果（例如一条规则））</li>
<li>学习算法：从数据中产生“模型”的算法 </li>
</ul>
<h2 id="二，基本术语"><a href="#二，基本术语" class="headerlink" title="二，基本术语"></a>二，基本术语</h2><ul>
<li>数据集</li>
<li>示例/样本/特征向量</li>
<li>属性/特征</li>
<li>属性值</li>
<li>属性空间/样本空间/输入空间：所有样本的集合</li>
<li>输出空间/标记空间：所有标记的集合</li>
<li>维数（属性的个数）</li>
<li>学习/训练：从数据中学得模型的过程</li>
<li>模型/学习器/假设：用训练集学习到的模型亦称为假设（hypothesis）:假设的表示一旦确定，假设空间及其规模就确定下来了</li>
<li>版本空间：与训练集完全匹配的假设的集合，版本空间的产生是因为一般假设空间过大，而训练数据集较有限（小）所导致的。</li>
<li>泛化能力：学的模型适用于新样本的能力</li>
</ul>
<h2 id="三，假设空间"><a href="#三，假设空间" class="headerlink" title="三，假设空间"></a>三，假设空间</h2><ul>
<li>归纳与演绎：是科学推理的两大基本手段。其中归纳是从特殊到一般的泛化过程，而演绎就是从一般到特殊的特化过程。在数学公理系统中，基于一组公里和推理规则推导出与之相洽的定理就是演绎。从样例中学习是一个归纳的过程，亦称为归纳学习</li>
<li>归纳学习有狭义和广义之分<ul>
<li>广义的归纳学习：相当于从样例中学习</li>
<li>狭义的归纳学习：要求从训练集中学的概念，因此也称为概念学习</li>
</ul>
</li>
</ul>
<h2 id="四，归纳偏好"><a href="#四，归纳偏好" class="headerlink" title="四，归纳偏好"></a>四，归纳偏好</h2><ul>
<li>归纳偏好/偏好：学习算法在学习过程中对某种类型假设的偏好， 任何一个有效的机器学习算法必有其归纳偏好 </li>
</ul>
<p>​    归纳偏好对应了学习算法本身所作出的关于“什么样的模型更好”的假设。</p>
<blockquote>
<p>这里解惑了我一直以来的疑惑：为什么模拟的图像是一个函数，为什么点与点之间的比较平缓呢</p>
<p>这是因为我们人类认为相似的样本应有相似的输出，所以学习算法得到的模型是一个函数，比较平滑、</p>
</blockquote>
<ul>
<li>奥卡姆剃刀：若有多个假设与观察一致，选择 最简单（曲线更平滑）的那个。</li>
<li>没有免费的午餐（NFL）：无论学习算法多聪明，或者多笨拙，他们的期望性能竟然相同。要具体问题具体分析。让我们知道学习算法的自身的归纳偏好与问题是否相配，往往会起到决定性的作用。</li>
</ul>
<h2 id="五，发展历程"><a href="#五，发展历程" class="headerlink" title="五，发展历程"></a>五，发展历程</h2><p>推理期 —&gt;  知识期 —&gt;  学习期</p>
<p>1950-1970 </p>
<h1 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章 模型评估与选择"></a>第二章 模型评估与选择</h1><h2 id="一，经验误差和过拟合"><a href="#一，经验误差和过拟合" class="headerlink" title="一，经验误差和过拟合"></a>一，经验误差和过拟合</h2><ul>
<li>错误率（error rate）：即如果在m个样本中有a个样本分类错误，则错误率为E = a/m</li>
<li>精度（accuracy）：精度=1-错误率</li>
<li>误差（error）：学习器在的实际预测输出与样本的真实输出之间的差异称为“误差”</li>
<li>训练误差/经验误差：学习器在训练集上的误差</li>
<li>泛化误差：在新样本上的误差</li>
<li>过拟合：把训练样本自身的一些不太一般的特点当作了所有潜在样本都会具有的一般性质，就会导致泛化性能下降。</li>
<li>欠拟合：对训练样本的一般性质尚未学好。如何克服：比较好克服，例如在决策树学习中扩展分支，在神经网络学习中增加训练轮数等。</li>
<li>模型选择（model selection）：该选哪一个学习算法，使用哪一种参数配置呢，这就是模型选择</li>
<li>分层采样：就是分配给无论是训练集还是测试集，其正负样本所占的比例都是相同的或近似的</li>
</ul>
<h2 id="二，评估方法"><a href="#二，评估方法" class="headerlink" title="二，评估方法"></a>二，评估方法</h2><h3 id="1-为什么要有评估方法："><a href="#1-为什么要有评估方法：" class="headerlink" title="1. 为什么要有评估方法："></a>1. 为什么要有评估方法：</h3><ul>
<li>使用一个测试集来测试学习器对新样本的判别能力，将测试集上的“测试误差”近似泛化误差。</li>
</ul>
<h3 id="2-常见方法"><a href="#2-常见方法" class="headerlink" title="2. 常见方法"></a>2. 常见方法</h3><ul>
<li><p>留出法（hand-out）：将整个数据集划分为两个互斥的集合，一个用来作为训练集，一个用来作为测试集。</p>
<ul>
<li>窘境：若训练集分配的较大，那么测试集就会较小，导致评估结果不够准确。若训练集分配的不够多，那么训练出的模型就会比整个数据集训练出来的模型有较大差别，从而降低了评估结果的保真性。</li>
</ul>
</li>
<li><p>交叉验证法/k折交叉验证：将数据集分为k组相同的分组（采用分层采样），将k-1组用于训练集，剩余一组用于测试集，这样就可以进行k次训练和测试，最终返回的是这k个测试结果的均值。</p>
<ul>
<li><p>留一法：当整个数据集为m个样本，若令k=m, 那么就得到了交叉验证法的一个特例：留一法</p>
<p>有一百万个样本，就要训练一百万个模型。</p>
</li>
</ul>
</li>
<li><p>自助法（bootstrapping）：以自助采样法（bootstrap sampling）为基础，在数据集较小，难以有效划分训练集和测试集时很有用。</p>
</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212122203843.png" alt="image-20221212220344621"></p>
<h3 id="3-调参与最终模型"><a href="#3-调参与最终模型" class="headerlink" title="3. 调参与最终模型"></a>3. 调参与最终模型</h3><ul>
<li>对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果。</li>
<li>而参数的范围是实数，所以要设置一个参数范围和变化步长。例如在[0, 0.2]范围中以0.05为步长</li>
<li>假定算法有三个参数，每个参数仅考虑5个候选值，这样对每一组训练/测试集就有5^3=125个模型需考察，工作量非常大。</li>
<li>模型评估与选择完成后，学习算法和参数配置已选定，此时应该用整个数据集重新训练模型。这个模型才是我们最终提交给用户的模型。</li>
<li>模型评估与选择中用于评估测试的数据集常称为“验证集”（validation set）。基于验证集上的性能来进行模型选择和调参。</li>
</ul>
<h2 id="三，性能度量"><a href="#三，性能度量" class="headerlink" title="三，性能度量"></a>三，性能度量</h2><ul>
<li>定义：衡量模型泛化能力的评价标准</li>
</ul>
<blockquote>
<p>回归任务最常用的性能度量是”均方误差” (mean squared error)  :</p>
<p>$ E(f;D)=\dfrac{1}{m}\sum_{i=1}^m\left(f\left(\boldsymbol{x}_i\right)-y_i\right)^2$ </p>
<p>更一般的，对于数据分布 和概率密度函数 p(.) 均方误差可描述为</p>
<p>$ E(f;\mathcal{D})=\int_{\boldsymbol{x}\sim\mathcal{D}}\left(f\left(\boldsymbol{x}\right)-y\right)^2p(\boldsymbol{x})\text{d}\boldsymbol{x}$ </p>
</blockquote>
<h3 id="1-错误率与精度"><a href="#1-错误率与精度" class="headerlink" title="1. 错误率与精度"></a>1. 错误率与精度</h3><ul>
<li>错误率：$E(f;D)=\dfrac{1}{m}\sum<em>{i=1}^m\mathbb{I}\left(f\left(\boldsymbol{x}_i\right)\neq y_i\right)$    对于数据分布D和概率密度函数p(.)， $E(f;{\cal D})=\int</em>{\mathcal{x}\sim{\cal D}}\mathbb{I}\left(f\left(\boldsymbol{x}\right)\neq y\right)p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\,$  </li>
<li>精度：$\begin{array}{rcl}\operatorname{acc}(f;D)&amp;=&amp;\dfrac{1}{m}\sum<em>{i=1}^m\mathbb{I}\left(f\left(\boldsymbol{x}_i\right)=y_i\right)\ &amp;=&amp;1-E(f;D)\:.\end{array}$    对于数据分布D和概率密度函数p(.)。 $\begin{array}{rcl}\operatorname{acc}(f;\mathcal{D})&amp;=&amp;\int</em>{\boldsymbol{x}\sim\mathcal{D}}\mathbb{I}\left(f\left(\boldsymbol{x}\right)-y\right)p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\ &amp;=&amp;1-E(f;\mathcal{D})\:.\end{array}$ </li>
</ul>
<h3 id="2-准确率（精准率-查准率）precision，召回率（查全率-recall）与F1"><a href="#2-准确率（精准率-查准率）precision，召回率（查全率-recall）与F1" class="headerlink" title="2. 准确率（精准率/查准率）precision，召回率（查全率 recall）与F1"></a>2. 准确率（精准率/查准率）precision，召回率（查全率 recall）与F1</h3><blockquote>
<p>错误率：有多少比例的瓜被判别错误</p>
<p>但是若我们关心的是“挑出的西瓜中有多少比例是好瓜”，或者“所有好瓜中有多少比例被挑了出来”。那么错误率显然就不够用了。</p>
</blockquote>
<p>检索出的信息中有多少比例是用户感兴趣的：这个是精准率</p>
<p>用户感兴趣的信息中有多少被检索出来了：这个是召回率</p>
<p>总结：一般precision和recall是矛盾的，此消彼长</p>
<ul>
<li>P-R曲线：以precision为纵轴，以recall为横轴</li>
<li>P-R图：就是以P-R曲线形成的图</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212131311382.png" alt="image-20221213131154349"></p>
<p>若一个曲线(A)包住了另一条曲线(C)，则认为A的学习器优于B的学习器</p>
<p>而A和B发生了交叉，这种情况就很难分清孰优孰劣，</p>
<p>这时的比较方法有：</p>
<ul>
<li>可以使用这些曲线所围成的面积，面积越大越优</li>
<li>可以通过平衡点，可知A优于B</li>
</ul>
<p>平衡点（BEP）：precision=recall</p>
<p>precision = TP / TP + FP<br>recall = TP / TP + FN</p>
<p>F1：平衡点的升级版  F1 = 2PR/(P+R)</p>
<h3 id="3-ROC和AUC"><a href="#3-ROC和AUC" class="headerlink" title="3. ROC和AUC"></a>3. ROC和AUC</h3><p>测试样本</p>
<ul>
<li>ROC：y轴为TPR，x轴为FPR<ul>
<li>TPR = TP/(TP + FN)</li>
<li>FPR = FP/(FP + TN)</li>
</ul>
</li>
<li>AUC：ROC曲线下的面积</li>
</ul>
<h3 id="4-代价敏感错误率与代价曲线"><a href="#4-代价敏感错误率与代价曲线" class="headerlink" title="4. 代价敏感错误率与代价曲线"></a>4. 代价敏感错误率与代价曲线</h3><p> <img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141127659.png" alt="image-20221214112731541"></p>
<ul>
<li><p>代价敏感错误率</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141128377.png" alt="image-20221214112836330"></p>
</li>
<li><p>代价曲线</p>
<ul>
<li>在非均等代价下，ROC曲线不能直接反映出学习器的<strong>期望总体代价</strong>，而代价曲线可以</li>
<li>x轴是正例概率代价。y轴式归一化代价</li>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141132534.png" alt="image-20221214113202496"></li>
<li>p: 就是样例本身就是正例的概率</li>
<li>FNR：假反例率 FNR = 1 - TPR</li>
<li>ROC曲线上的一点对应了代价平面的一条线段</li>
<li>期望总体代价在代价曲线上的求解步骤：<ul>
<li>设ROC曲线上的点的坐标为（FPR，TPR），则可计算出相应的FNR。然后再代价平面上绘制一条从（0，FPR）到（1，FNR）的线段，则一条线段下的面积表示了该条件下的期望总体代价</li>
<li>将ROC曲线上的每个点都转化为代价平面上的一条线段</li>
<li>然后取所有线段的下界的交集，围成的图形面积 == 在所有条件下学习器的期望总体代价。</li>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141141216.png" alt="image-20221214114123180"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="四，比较检验"><a href="#四，比较检验" class="headerlink" title="四，比较检验"></a>四，比较检验</h2><p>先使用某种实验评估方法，测得学习期的某个性能度量结果，然后对这些结果进行比较</p>
<p>那怎么进行比较呢？且听我娓娓道来！为了方便，下面都使用<strong>错误率</strong>来作为性能度量 用$\epsilon$ 表示</p>
<h3 id="1-假设检验"><a href="#1-假设检验" class="headerlink" title="1. 假设检验"></a>1. 假设检验</h3><ul>
<li>假设：对学习器泛化错误率分布的某种判断或猜想 如：$\epsilon=\epsilon_{0}$   ，现实任务中我们并不知道学习器的泛化错误率 $\epsilon$。只能获知其测试错误率 $\hat{\epsilon}$ </li>
<li>泛化错误率未必与测试错误率相同，但是可根据测试错误率 $\hat{\epsilon}$  估推出泛化错误率的分布</li>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141518366.png" alt="image-20221214151843325"></li>
<li>令$\partial P(\hat{\epsilon};\epsilon)/\partial\epsilon=0$   ；解的 $\epsilon=\hat{\epsilon}$，即<img src="../../../../AppData/Roaming/Typora/typora-user-images/image-20221214153242382.png" alt="image-20221214153242382"> 时最大。</li>
<li>结论：若$\epsilon$ = 0.3，则10个样本中测得3个被误分类的概率最大。      </li>
<li>检验方法：<ul>
<li>这两个检验方法都是对单个的学习器泛化性能的假设进行检验。</li>
<li>二项检验: 如下图<ul>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141633263.png" alt="image-20221214163337225"></li>
</ul>
</li>
<li>t检验：<ul>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141635275.png" alt="image-20221214163513232"></li>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141640173.png" alt="image-20221214164011128"></li>
<li><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212141635039.png" alt="image-20221214163540996"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-交叉验证t检验"><a href="#2-交叉验证t检验" class="headerlink" title="2. 交叉验证t检验"></a>2. 交叉验证t检验</h3><p>p40</p>
<h3 id="3-MCNemar检验"><a href="#3-MCNemar检验" class="headerlink" title="3. MCNemar检验"></a>3. MCNemar检验</h3><p>p41</p>
<h3 id="Freedman检验与Nemenyi检验"><a href="#Freedman检验与Nemenyi检验" class="headerlink" title="Freedman检验与Nemenyi检验"></a>Freedman检验与Nemenyi检验</h3><p>p42</p>
<h2 id="五，偏差与方差"><a href="#五，偏差与方差" class="headerlink" title="五，偏差与方差"></a>五，偏差与方差</h2><p>偏差方差分解(bias-variance decomposition) 是解释<strong>学习算法泛化性能</strong>的一种重要工具.</p>
<p>偏差方差分解试图对学习算法的<strong>期望泛化错误率</strong>进行拆解.</p>
<p>对测试样本x，令 $y_D$ 在数据集中的标记， y为x的真实标记， <em>f(x;</em> D) 为训练集D上学得模型 f在x上的预测输出</p>
<ul>
<li><p>学习算法的期望预测为：$\bar{f}(\dot{\boldsymbol{x}})=\mathbb{E}_D[f(\boldsymbol{x};D)]$ </p>
</li>
<li><p>使用样本数相同的不同训练集产生的方差为:  $var(\boldsymbol{x})=\mathbb{E}_D\left[\left(f\left(\boldsymbol{x};D\right)-\bar{f}\left(\boldsymbol{x}\right)\right)^2\right]$ </p>
</li>
<li><p>噪声为：$\varepsilon^2=\mathbb{E}_D\left[\left(y_D-y\right)^2\right]$ </p>
</li>
<li><p>期望输出与真实标记的差别称为偏差(bias) ，即: $ bias^2(\boldsymbol{x})=\left(\bar{f}(\boldsymbol{x})-y\right)^2$ 1</p>
</li>
<li><p>为便于讨论，假定噪声期望为0，即 $\mathbb{E}_D\left[y_D-y\right]$  = 0通过简单的多项式展开合</p>
<p>井，可对算法的期望泛化误差进行分解， 也就是推导公式呗:</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212212243674.png" alt="image-20221221224327562"></p>
</li>
</ul>
<h3 id="偏差一方差窘境"><a href="#偏差一方差窘境" class="headerlink" title="偏差一方差窘境"></a>偏差一方差窘境</h3><p>一般来说，偏差与方差是有冲突的，这称为偏差一方差窘境(bias-variance </p>
<p>dilemma).下图给出了一个示意图，给定学习任务，假定我们能控制学习算法</p>
<p>的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足</p>
<p>以便学习器产生显著变化，此时偏差主导了泛化错误率;随着训练程度的加深，</p>
<p>学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差</p>
<p>逐渐主导了泛化错误率;在训练程度充足后，学习器的拟合能力已非常强，训练</p>
<p>数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全</p>
<p>局的特性被学习器学到了，则将发生过拟合.</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212212249060.png" alt="image-20221221224941000"></p>
<h1 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h1><h2 id="1-基本形式"><a href="#1-基本形式" class="headerlink" title="1. 基本形式"></a>1. 基本形式</h2><p>由 个属性描述的示例 $ \boldsymbol{x}=(x_1;x_2;\ldots;x_d)$  其中$x_i$是 $x$在第 i 个属性上的取值，线性模型(linear model) 试图学得一个通过属性的线性组合来进行预测的函数，即：$ f(\boldsymbol{x})=w_1x_1+w_2x_2+\ldots+w_dx_d+b$ ，</p>
<p>一般用向量形式写成：$ f(\boldsymbol{x})=\boldsymbol{w}^\text{T}\boldsymbol{x}+b$  ，</p>
<p>其中 $ \boldsymbol{w}=(w_1;w_2;\ldots;w_d)$  $ \boldsymbol{w}, b$学得之后，模型就得以确定.</p>
<p>线性模型有很好的<strong>可解释性</strong> (comprehensibility) .例如若在西瓜问题中学得 “f好瓜 (x)=0.2 x色泽 +O.5 x根蒂 +O.3 x敲声 +1” ，则意味着可通过综合考虑色泽、根蒂和敲声来判断瓜好不好，其中根蒂最要紧，而敲声比色泽更重要.</p>
<h2 id="2-线性回归"><a href="#2-线性回归" class="headerlink" title="2. 线性回归"></a>2. 线性回归</h2><p>给定数据集$D={(\boldsymbol{x}<em>1,y_1),(\dot{\boldsymbol{x}}_2,y_2),\ldots,(\boldsymbol{x}_m,y_m)}$ , 其中$x</em>{i}=(x<em>{i1},x</em>{i2};\ldots,x<em>{i</em>{d}}) $ , $y_i\in\mathbb{R}$ . </p>
<p>对离散属性，若属性值间存在”序” (order) 关系，可通过连续化将其转化为连续值，例如二值属性”身高”的取值”高” “矮”可转化为 {1 O. 叶，三值属性”高度”的取值”高” “中” “低”可转化为 {1 0.5 0.0}; 若属性值间不存在序关系，假定有 k个属性值（一个属性所能取的值的个数为k），则通常转化为 k维向量，例如属性”瓜类”的取值”西瓜” “南瓜” “黄瓜”可转化为 (0 0 1) (0, 1,0) 和 (1,0,0)</p>
<p>线性回归试图学得：$ f(x<em>i)=wx_i+b$ 使得 $ f(x</em>{i})\simeq y_{i}\text{}$  </p>
<p>那么怎么确定w 和 b呢？显然?关键在于如何衡量 f(x) 与 y 之间的差别 .均方误差（square loss）是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即</p>
<script type="math/tex; mode=display">
\begin{aligned}(w^*,b^*)&=\underset{(w,b)}{\text{arg min}}\sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2\\ &=\underset{(w,b)}{\text{arg min}}\sum_{i=1}^m\left(y_i-wx_i-b\right)^2\end{aligned}</script><p>$ 注 ： w^{<em>},b^{</em>} 是w 和b 的解$ </p>
<p>基于均方误差最小化来进行模型求解的方法称为”最小二乘法” (least square method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.</p>
<ul>
<li>以下是求解$ w, b$ 的推导过程</li>
</ul>
<p>$ \dfrac{\partial E<em>{(w,b)}}{\partial w}=2\left(w\sum</em>{i=1}^{m}x<em>i^2-\sum</em>{i=1}^{m}\left(y_i-b\right)x_i\right)$ </p>
<p>$ \dfrac{\partial E<em>{(w,b)}}{\partial b}=2\left(mb-\sum</em>{i=1}^m\left(y_i-wx_i\right)\right)$ </p>
<p>然后令上两式为零可得到最优解的闭式(closed-form)解：$w=\dfrac{\sum\limits<em>{i=1}^{m}y_i(x_i-\bar x)}{\sum\limits</em>{i=1}^{m}x<em>i^2-\frac{1}{m}\left(\sum\limits</em>{i=1}^{m}x<em>i\right)^2}$    $ b=\dfrac{1}{m}\sum</em>{i=1}^m(y_i-wx_i)$</p>
<p>其中$ \bar{x}=\frac{1}{m}\sum\limits_{i=1}^{m}x_i$ 是x的均值.</p>
<blockquote>
<p>疑惑 ; 这个w怎么解不出来，待会再看，b能解出来，是不是将b的解出来的结果带入到w的式子中，但是就是化简不了w到上式的样子。再看看吧， 哎，我太菜了哎。</p>
</blockquote>
<p>更一般的情形是如本节开头的数据集D 样本由 d个属性描述.此时我们试图学得 $ f(\boldsymbol{x}_i)=\boldsymbol{w}^\text{T}\boldsymbol{x}_i+b$ , 使得$ f(\boldsymbol{x}_i)\simeq y_i$ , 这称为”多元线性回归”</p>
<p>类似的，可利用最小二乘法来对w, b 进行估计.为便于讨论，我们把吸收入向量形式 $\hat{\boldsymbol{w}}=(\boldsymbol{w};b)$ 相应的，把数据集 D表示为一个 m×(d+1)大小的矩阵X ，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1 ，即：$ \mathbf{X}=\begin{pmatrix}x<em>{11}&amp;x</em>{12}&amp;\ldots&amp;x<em>{14}&amp;1\ x</em>{21}&amp;x<em>{22}&amp;\ldots&amp;x</em>{24}&amp;1\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\ x<em>{m1}&amp;x</em>{m2}&amp;\ldots&amp;x<em>{md}&amp;1\end{pmatrix}=\begin{pmatrix}\mathbf{x}</em>{1}^{\mathrm{T}}&amp;1\ \mathbf{x}<em>{2}^{\mathrm{T}}&amp;1\ \vdots&amp;\vdots\ \mathbf{x}</em>{m}^{\mathrm{T}}&amp;1\end{pmatrix}$</p>
<p>再把标记也写成向量形式$ \boldsymbol{y}=(y_1;y_2;\ldots;y_m)$ , 有 $ \hat{\boldsymbol{w}}^{\ast}=\underset{\boldsymbol{\hat{w}}}{\mathrm{argmin}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)^{\mathrm{T}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)$ </p>
<p>令 $ E<em>{\boldsymbol{\hat{w}}}=\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)^{\mathrm{T}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)$ ,J)，对求导得到: $ \dfrac{\partial E</em>{\hat{\boldsymbol{w}}}}{\partial\hat{\boldsymbol{w}}}=2\mathbf{X}^{\text{T}}\left(\mathbf{X}\hat{\boldsymbol{w}}-\boldsymbol{y}\right)$ </p>
<p>令上式为零可得$ \hat{\boldsymbol{w}}$ 最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些.下面我们做一个简单的讨论.</p>
<p>$ \mathbf{X^{\text{T}}}\mathbf{X}$为满秩矩阵(full-rank matrix) 或正走矩阵(positive definite ma</p>
<p>trix) 时，令式为零可得：$ \hat{\boldsymbol{w}}^*=\left(\mathbf{X}^{\text{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\text{T}}\boldsymbol{y}$ ， 令$ \hat{\boldsymbol{x}}<em>{i}=(\dot{\boldsymbol{x}}</em>{i},1)$ ,则最终学得的多元线性回归模型为 $ f(\hat{\boldsymbol{x}}_i)=\hat{\boldsymbol{x}}_i^{\mathrm T}\left(\mathbf{X}^{\mathrm T}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathrm T}\boldsymbol{y}$ </p>
<p>然而，现实任务中 $  \mathbf{X^{\text{T}}}\mathbf{X}$ 往往不是满秩矩 .例如在许多任务中我们会遇到大量的变量?其数目甚至超过样例数，导致 的列数多于行$  \mathbf{X^{\text{T}}}\mathbf{X}$  显然不秩.此时可解出多个$\hat{\boldsymbol{w}}$ 它们都能使均方误差 小化.选择哪 个解作为输出，将由学习算法的归纳偏好决定， 常见的做法是引入正则化 (regularization) 项.</p>
<ul>
<li>线性模型：$f(\boldsymbol{x})=\boldsymbol{w}^\text{T}\boldsymbol{x}+b$</li>
<li>对数线性模型：$\ln y=\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b$</li>
<li>广义线性模型：$y=g^{-1}(\boldsymbol{w}^{\text{T}}\boldsymbol{x}+b)$   s.t. 单调可微函数 g(.)</li>
</ul>
<p>显然 对数线性回归 是广义线性模型 g(.) = ln(.) 时的特例.</p>
<h2 id="3-对数几率回归"><a href="#3-对数几率回归" class="headerlink" title="3. 对数几率回归"></a>3. 对数几率回归</h2><p>待学  p57  等什么时候用上了再看吧</p>
<h2 id="5-多分类学习"><a href="#5-多分类学习" class="headerlink" title="5. 多分类学习"></a>5. 多分类学习</h2><p>不失一般性，考虑 N个类别$\text{}C_1,C_2,\ldots,C_N$多分类学习的基本思路是”拆解法”，即将<strong>多分类任务</strong>拆为<strong>若干个二分类任务</strong>求解，关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集成. 所以要怎么实行一个策略来进行拆分和集成</p>
<ul>
<li>最经典的三种拆分策略，这里的一是说类别的数量<ul>
<li>一对一（One vs. One, 简称 OvO）</li>
<li>一对其余（One vs. Rest,简称OvR）<ul>
<li>在测试时若<strong>仅有一个</strong>分类器预测为正类，则对应的类别标记作为最终分类结果，如下图所示.若有多个分类器预测为正类 则通常考虑<strong>各预测为正类的分类器的预测置信度</strong>，选择<strong>置信度最大</strong>的类别标记作为分类结果.</li>
</ul>
</li>
<li>多对多（Many vs. Many,简称MvM）</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212221524837.png" alt="image-20221222152415714"></p>
<ul>
<li><p>OvO和OvR的对比</p>
<p>OvR 只需训练 N个分类器, 而 OvO 需训练 N(N - 1)/2 个分类器 因此， OvO的存储开销和测试时间开销通常比 OvR 更大 但在训练时，OvR 的每个分类器均使用全部训练样例，而 OvO 的每个<strong>分类器</strong>仅用到<strong>两个类的样</strong>例，因此，在类别很多时， OvO 的训练时间开销通常比 OvR 更小 至于预测性能 则取决于具体的数据分布， 在多数情形下两者差不多.</p>
</li>
</ul>
<h1 id="第四章-决策树"><a href="#第四章-决策树" class="headerlink" title="第四章 决策树"></a>第四章 决策树</h1><h2 id="1-基本流程"><a href="#1-基本流程" class="headerlink" title="1. 基本流程"></a>1. 基本流程</h2><p>叶节点对应决策结果，其他节点对应一个属性测试</p>
<h1 id="第五章-神经网络"><a href="#第五章-神经网络" class="headerlink" title="第五章 神经网络"></a>第五章 神经网络</h1><h1 id="第六章-支持向量机"><a href="#第六章-支持向量机" class="headerlink" title="第六章 支持向量机"></a>第六章 支持向量机</h1><h2 id="1-间隔和支持向量"><a href="#1-间隔和支持向量" class="headerlink" title="1. 间隔和支持向量"></a>1. 间隔和支持向量</h2><p>给定训练样本集$D={(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\ldots,(\boldsymbol{x}_m,y_m)}$ , $y_i\in{-1,+1}$ </p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212251515031.png" alt="image-20221225151544886"></p>
<p>选择中间这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强.</p>
<p>划分超平面 可以表示为：$\boldsymbol{w}^{\text{T}}\boldsymbol{x}+b=0$  </p>
<blockquote>
<p>其中 $\boldsymbol{w}=(w_1;w_2;\ldots;w_d)$ 为平面的法向量，决定了划分超平面的的方向</p>
<p>b 为位移项，决定了超平面与原点之间的距离.</p>
<p>超平面由$\boldsymbol{w}$和b共同决定, 就将划分超平面简单记为($w, b$) </p>
</blockquote>
<p>样本空间中任意点 x到超平面（$w, b$）的距离可写为: $\quad r=\dfrac{|\boldsymbol{w}^\text{T}\boldsymbol{x}+b|}{||\boldsymbol{w}||}$  (6.2)</p>
<p>假设超平面（$w, b$）能将训练样本正确分类，即对于$\quad(\boldsymbol{x}_i,y_i)\in D$ , 若$y_i$ =+1，则有$\boldsymbol{w}^\text{T}\boldsymbol{x}_i+b&gt;0$ ; 若$y_i$ = -1, 则有 $\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i+b&lt;0$ .</p>
<p>$\left{\begin{array}{ccc}\boldsymbol{w}^\text{T}\boldsymbol{x}_i+b\geqslant+1,&amp;y_i=+1;\ \boldsymbol{w}^\text{T}\boldsymbol{x}_i+b\leqslant-1,&amp;y_i=-1.\end{array}\right.$   (6.3)</p>
<ul>
<li><p>支持向量（support vector）：距离超平面最近的这几个训练样本点使式（6.3）的等号成立的这几个训练样本</p>
</li>
<li><p>间隔$\gamma$（margin）：两个异类支持向量到超平面的距离之和为</p>
<p>$\gamma=\dfrac{2}{||\boldsymbol{w}||}$  (6.4)</p>
</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212251540310.png" alt="image-20221225154055203"></p>
<p>欲找到具有”最大间隔” (maximum margin) 的划分超平面，也就是要找到能满足式(6.3) 中约束的参数$w和b$ 使得 $\gamma$最大，也就是间隔最大，即</p>
<p>  $\begin{array}{ccc}\max_{\boldsymbol{w},b}&amp;\dfrac{2}{||\boldsymbol{w}||}\end{array}.$   (6.5)</p>
<p>s.t. $\quad y_i(\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i+b)\geqslant1,\quad i=1,2,\ldots,m.$      </p>
<p>显然（6.5）等价于</p>
<p>$\begin{array}{ll}\min\limits_{\boldsymbol{w},b}&amp;\dfrac12|\boldsymbol{w}|^2\end{array}$    （6.6）</p>
<p>s.t. $\quad y_i(\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i+b)\geqslant1,\quad i=1,2,\ldots,m.$ </p>
<p>这就是支持向量机（SVM，Support Vector Machine）的基本型</p>
<h2 id="2-对偶问题"><a href="#2-对偶问题" class="headerlink" title="2. 对偶问题"></a>2. 对偶问题</h2><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212251649943.png" alt="image-20221225164955819"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212251652422.png" alt="image-20221225165202086"></p>
<h1 id="第七章-贝叶斯分类器"><a href="#第七章-贝叶斯分类器" class="headerlink" title="第七章 贝叶斯分类器"></a>第七章 贝叶斯分类器</h1><h1 id="第八章-集成学习"><a href="#第八章-集成学习" class="headerlink" title="第八章 集成学习"></a>第八章 集成学习</h1><p>集成学习分为（代表如下）：</p>
<ul>
<li>Boosting：<ul>
<li>个体学习器问存在强依赖关系、必须串行生成的序列化方法</li>
</ul>
</li>
<li>Bagging 和随机森林<ul>
<li>个体学习器间不存在强依赖关系、可同时生成的并行化方法;</li>
</ul>
</li>
</ul>
<h2 id="1-个体与集成"><a href="#1-个体与集成" class="headerlink" title="1. 个体与集成"></a>1. 个体与集成</h2><blockquote>
<p>个体学习器/组件学习器</p>
<p>.同质集成中的个体学习器亦称”基学习器”,相应的学习算法称为”基学习算法”</p>
<p>集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是”异质”的 (heterogenous) .异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法;相应的，个体学习器一般不称为基学习器，常称为”组件学习器(component learner) 或直接称为个体学习器.</p>
</blockquote>
<p>​    <img src="https://gitee.com/fjkf/images/raw/master/imgs/202212261112036.png" alt="image-20221226111238992"></p>
<h2 id="2-Boosting"><a href="#2-Boosting" class="headerlink" title="2. Boosting"></a>2. Boosting</h2><ul>
<li>Boosting 是一族可将弱学习器提升为强学习器的算法</li>
<li>算法工作机制大体类似:  先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值T, 最终将这 T个基学习器进行加权结合.</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212262005917.png" alt="image-20221226200539800"></p>
<h1 id="第九章-聚类"><a href="#第九章-聚类" class="headerlink" title="第九章 聚类"></a>第九章 聚类</h1><h2 id="1-聚类任务"><a href="#1-聚类任务" class="headerlink" title="1. 聚类任务"></a>1. 聚类任务</h2><p>无监督学习中最常见的学习任务就是“聚类（clustering）”</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212301649970.png" alt=""></p>
<p>聚类面临的两个基本问题：</p>
<ul>
<li>性能度量</li>
<li>距离计算</li>
</ul>
<h2 id="2-性能度量"><a href="#2-性能度量" class="headerlink" title="2. 性能度量"></a>2. 性能度量</h2><p>簇内相似度高且 簇间相似度低</p>
<p>聚类性能度量大致有两类. </p>
<ul>
<li>外部指标：一类是将聚类结果与某个”参考模型” (reference model) 进行比较，称为”外部指标” (external dex); </li>
<li>内部指标：另一类是直接考察聚类结果而不利用任何参考模型，称为”内部指标” (internal index). </li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212301650754.png" alt="image-20221230165030569"></p>
<h3 id="2-1-聚类性能度量外部指标"><a href="#2-1-聚类性能度量外部指标" class="headerlink" title="2.1 聚类性能度量外部指标"></a>2.1 聚类性能度量外部指标</h3><h4 id="Jaccard-系数"><a href="#Jaccard-系数" class="headerlink" title="Jaccard 系数"></a>Jaccard 系数</h4><p>$\text{JC}=\dfrac{a}{a+b+\dot{c}}.$ </p>
<p>(Jaccard Coefficient ，简称 JC)</p>
<h4 id="FM-指数"><a href="#FM-指数" class="headerlink" title="FM 指数"></a>FM 指数</h4><p>$\text{FMI}=\sqrt{\dfrac{a}{a+b}:\dfrac{a}{a+c}}$ </p>
<p>(Fowlkes and Mallows lndex ，简称 FMI)</p>
<h4 id="Rand-指数"><a href="#Rand-指数" class="headerlink" title="Rand 指数"></a>Rand 指数</h4><p>$\text{RI}=\dfrac{2(a+d)}{m(m-1)}$ </p>
<p>(Rand Index ，简称 RI)</p>
<p>显然，上述性能度量的结果值均在 [0 1] 区间，值越大越好.</p>
<h3 id="2-2-聚类性能度量内部指标"><a href="#2-2-聚类性能度量内部指标" class="headerlink" title="2.2 聚类性能度量内部指标"></a>2.2 聚类性能度量内部指标</h3><p><img src="https://gitee.com/fjkf/images/raw/master/imgs/202212301655821.png" alt="image-20221230165526697"></p>
<h4 id="DB-指数"><a href="#DB-指数" class="headerlink" title="DB 指数"></a>DB 指数</h4><p>(Davies-Bouldin Index，简称 DBI)</p>
<p>$\text{DBI}=\dfrac{1}{k}\sum<em>{i=1}^k\max</em>{j\neq i}\left(\dfrac{\text{avg}(C<em>i)+\text{avg}(C_j)}{d</em>{\text{cen}}(\boldsymbol{\mu}_i,\boldsymbol{\mu}_j)}\right)$ </p>
<h4 id="Dunn-指数"><a href="#Dunn-指数" class="headerlink" title="Dunn 指数"></a>Dunn 指数</h4><p>(Dunn Index，简称 DI)</p>
<p> $\mathrm{DI}=\underset{1\leqslant i\leqslant k}{\text{min}}\left{\underset{j\neq i}{\text{min}}\left(\dfrac{d<em>{\text{min}}(C_i,C_j)}{\text{max}</em>{1\leqslant l\leqslant k}\text{diam}(C_l)}\right)\right}$ </p>
<p>显然， DBI 的值越小越好，而 DI 则相反，值越大越好.</p>
<h2 id="3-距离计算"><a href="#3-距离计算" class="headerlink" title="3. 距离计算"></a>3. 距离计算</h2><p>给定样本 Xi = (Xi1; Xi2;… ;Xin) 与的 X2=(Xj1; <em>Xj2;</em> . . . ; Xjn) </p>
<p>最常用的是”闵可夫斯基距离” (Minkowski distance) : $\text{dist}<em>{\text{mk}}(\boldsymbol{x}_i,\boldsymbol{x}_j)=\left(\sum</em>{u=1}^n|x<em>{iu}-x</em>{ju}|^p\right)^{\frac{1}{p}}$ , 也可以称为xi-xj的Lp范数 $||\boldsymbol{x}<em>{i}-\boldsymbol{x}</em>{j}||_{p}$</p>
<p>p=2 时，闵可夫斯基距离即欧氏距离(Euclidean distance)：$\text{dist}<em>{\text{ed}}(\boldsymbol{x}_i,\boldsymbol{x}_j)=||\boldsymbol{x}_i-\boldsymbol{x}_j||_2=\sqrt{\sum</em>{u=1}^n|\boldsymbol{x}<em>{iu}-\boldsymbol{x}</em>{ju}|^2}$ </p>
<p>p=1时，闵可夫斯基距离即曼哈顿距离(Manhattan distance) :  $\text{dist}<em>{\text{man}}(\boldsymbol{x}_i,\boldsymbol{x}_j)=||\boldsymbol{x}_i-\boldsymbol{x}_j||_1=\sum</em>{u=1}^n|\overset{n}{x<em>{iu}}-x</em>{ju}|$ </p>
<p>详见：p200-201</p>
<h2 id="4-原型聚类"><a href="#4-原型聚类" class="headerlink" title="4. 原型聚类"></a>4. 原型聚类</h2><h1 id="降维与度量学习"><a href="#降维与度量学习" class="headerlink" title="降维与度量学习"></a>降维与度量学习</h1><h1 id="特征选择与稀疏学习"><a href="#特征选择与稀疏学习" class="headerlink" title="特征选择与稀疏学习"></a>特征选择与稀疏学习</h1><h1 id="计算学习理论"><a href="#计算学习理论" class="headerlink" title="计算学习理论"></a>计算学习理论</h1><h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><h1 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h1><h1 id="规则学习"><a href="#规则学习" class="headerlink" title="规则学习"></a>规则学习</h1><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://wdpname.github.io">dpWu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wdpname.github.io/2022/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E5%91%A8%E5%BF%97%E5%8D%8E%E7%89%88%EF%BC%89/">https://wdpname.github.io/2022/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%88%E5%91%A8%E5%BF%97%E5%8D%8E%E7%89%88%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wdpname.github.io" target="_blank">平博社</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML/">ML</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/01/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%88%B7leetcode/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">从零开始刷leetcode</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/29/PAT-Basic-Level-Practice-%E4%B8%AD%E6%96%87/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PAT(Basic Level)Practice(中文)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/01/08/kmeans%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93/" title="kmeans聚类算法的总结"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-08</div><div class="title">kmeans聚类算法的总结</div></div></a></div><div><a href="/2023/02/14/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA%E8%AE%B2%E8%A7%A3/" title="主成分分析PCA讲解"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-14</div><div class="title">主成分分析PCA讲解</div></div></a></div><div><a href="/2022/12/22/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B/" title="《机器学习实战》"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-22</div><div class="title">《机器学习实战》</div></div></a></div><div><a href="/2023/02/22/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%90%88%E9%9B%86%E4%B8%8E%E7%90%86%E8%A7%A3/" title="优化算法合集与理解"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-22</div><div class="title">优化算法合集与理解</div></div></a></div><div><a href="/2022/12/19/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/" title="名词解释"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-19</div><div class="title">名词解释</div></div></a></div><div><a href="/2023/01/03/%E5%AF%B9%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E7%8E%B0%E7%AE%97%E6%B3%95%E7%9A%84%E6%84%9F%E6%82%9F/" title="对于朴素贝叶斯实现算法的感悟"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-03</div><div class="title">对于朴素贝叶斯实现算法的感悟</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dpWu</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">160</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdpname" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://1939317922@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">机器学习分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%AC%A6%E5%8F%B7%E8%A1%A8"><span class="toc-number">2.</span> <span class="toc-text">主要符号表</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA"><span class="toc-number">3.</span> <span class="toc-text">第一章 绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%EF%BC%8C%E5%BC%95%E8%A8%80"><span class="toc-number">3.1.</span> <span class="toc-text">一，引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%EF%BC%8C%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD"><span class="toc-number">3.2.</span> <span class="toc-text">二，基本术语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%EF%BC%8C%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4"><span class="toc-number">3.3.</span> <span class="toc-text">三，假设空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%EF%BC%8C%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD"><span class="toc-number">3.4.</span> <span class="toc-text">四，归纳偏好</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%EF%BC%8C%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-number">3.5.</span> <span class="toc-text">五，发展历程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">4.</span> <span class="toc-text">第二章 模型评估与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%EF%BC%8C%E7%BB%8F%E9%AA%8C%E8%AF%AF%E5%B7%AE%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.1.</span> <span class="toc-text">一，经验误差和过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%EF%BC%8C%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">二，评估方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">4.2.1.</span> <span class="toc-text">1. 为什么要有评估方法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.2.</span> <span class="toc-text">2. 常见方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%B0%83%E5%8F%82%E4%B8%8E%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.3.</span> <span class="toc-text">3. 调参与最终模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%EF%BC%8C%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="toc-number">4.3.</span> <span class="toc-text">三，性能度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%94%99%E8%AF%AF%E7%8E%87%E4%B8%8E%E7%B2%BE%E5%BA%A6"><span class="toc-number">4.3.1.</span> <span class="toc-text">1. 错误率与精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88%E7%B2%BE%E5%87%86%E7%8E%87-%E6%9F%A5%E5%87%86%E7%8E%87%EF%BC%89precision%EF%BC%8C%E5%8F%AC%E5%9B%9E%E7%8E%87%EF%BC%88%E6%9F%A5%E5%85%A8%E7%8E%87-recall%EF%BC%89%E4%B8%8EF1"><span class="toc-number">4.3.2.</span> <span class="toc-text">2. 准确率（精准率&#x2F;查准率）precision，召回率（查全率 recall）与F1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-ROC%E5%92%8CAUC"><span class="toc-number">4.3.3.</span> <span class="toc-text">3. ROC和AUC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87%E4%B8%8E%E4%BB%A3%E4%BB%B7%E6%9B%B2%E7%BA%BF"><span class="toc-number">4.3.4.</span> <span class="toc-text">4. 代价敏感错误率与代价曲线</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%EF%BC%8C%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C"><span class="toc-number">4.4.</span> <span class="toc-text">四，比较检验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C"><span class="toc-number">4.4.1.</span> <span class="toc-text">1. 假设检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81t%E6%A3%80%E9%AA%8C"><span class="toc-number">4.4.2.</span> <span class="toc-text">2. 交叉验证t检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-MCNemar%E6%A3%80%E9%AA%8C"><span class="toc-number">4.4.3.</span> <span class="toc-text">3. MCNemar检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Freedman%E6%A3%80%E9%AA%8C%E4%B8%8ENemenyi%E6%A3%80%E9%AA%8C"><span class="toc-number">4.4.4.</span> <span class="toc-text">Freedman检验与Nemenyi检验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%EF%BC%8C%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-number">4.5.</span> <span class="toc-text">五，偏差与方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%80%E6%96%B9%E5%B7%AE%E7%AA%98%E5%A2%83"><span class="toc-number">4.5.1.</span> <span class="toc-text">偏差一方差窘境</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">第三章 线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">1. 基本形式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.2.</span> <span class="toc-text">2. 线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92"><span class="toc-number">5.3.</span> <span class="toc-text">3. 对数几率回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.4.</span> <span class="toc-text">5. 多分类学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.</span> <span class="toc-text">第四章 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">6.1.</span> <span class="toc-text">1. 基本流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">第五章 神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">8.</span> <span class="toc-text">第六章 支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%97%B4%E9%9A%94%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="toc-number">8.1.</span> <span class="toc-text">1. 间隔和支持向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">8.2.</span> <span class="toc-text">2. 对偶问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">9.</span> <span class="toc-text">第七章 贝叶斯分类器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.</span> <span class="toc-text">第八章 集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%AA%E4%BD%93%E4%B8%8E%E9%9B%86%E6%88%90"><span class="toc-number">10.1.</span> <span class="toc-text">1. 个体与集成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Boosting"><span class="toc-number">10.2.</span> <span class="toc-text">2. Boosting</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E8%81%9A%E7%B1%BB"><span class="toc-number">11.</span> <span class="toc-text">第九章 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%81%9A%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="toc-number">11.1.</span> <span class="toc-text">1. 聚类任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="toc-number">11.2.</span> <span class="toc-text">2. 性能度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%87"><span class="toc-number">11.2.1.</span> <span class="toc-text">2.1 聚类性能度量外部指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Jaccard-%E7%B3%BB%E6%95%B0"><span class="toc-number">11.2.1.1.</span> <span class="toc-text">Jaccard 系数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FM-%E6%8C%87%E6%95%B0"><span class="toc-number">11.2.1.2.</span> <span class="toc-text">FM 指数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rand-%E6%8C%87%E6%95%B0"><span class="toc-number">11.2.1.3.</span> <span class="toc-text">Rand 指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F%E5%86%85%E9%83%A8%E6%8C%87%E6%A0%87"><span class="toc-number">11.2.2.</span> <span class="toc-text">2.2 聚类性能度量内部指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DB-%E6%8C%87%E6%95%B0"><span class="toc-number">11.2.2.1.</span> <span class="toc-text">DB 指数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dunn-%E6%8C%87%E6%95%B0"><span class="toc-number">11.2.2.2.</span> <span class="toc-text">Dunn 指数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">11.3.</span> <span class="toc-text">3. 距离计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%8E%9F%E5%9E%8B%E8%81%9A%E7%B1%BB"><span class="toc-number">11.4.</span> <span class="toc-text">4. 原型聚类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">12.</span> <span class="toc-text">降维与度量学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">13.</span> <span class="toc-text">特征选择与稀疏学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-number">14.</span> <span class="toc-text">计算学习理论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">15.</span> <span class="toc-text">半监督学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="toc-number">16.</span> <span class="toc-text">概率图模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0"><span class="toc-number">17.</span> <span class="toc-text">规则学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">18.</span> <span class="toc-text">强化学习</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/15/Maven%E5%AE%9E%E6%88%98/" title="Maven实战"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Maven实战"/></a><div class="content"><a class="title" href="/2023/10/15/Maven%E5%AE%9E%E6%88%98/" title="Maven实战">Maven实战</a><time datetime="2023-10-15T11:12:27.000Z" title="发表于 2023-10-15 19:12:27">2023-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/15/%E5%9B%BE%E8%A7%A3HTTP/" title="图解HTTP"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="图解HTTP"/></a><div class="content"><a class="title" href="/2023/10/15/%E5%9B%BE%E8%A7%A3HTTP/" title="图解HTTP">图解HTTP</a><time datetime="2023-10-15T10:39:45.000Z" title="发表于 2023-10-15 18:39:45">2023-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/13/Spring-Cloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5/" title="Spring Cloud微服务和分布式系统实践"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spring Cloud微服务和分布式系统实践"/></a><div class="content"><a class="title" href="/2023/10/13/Spring-Cloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5/" title="Spring Cloud微服务和分布式系统实践">Spring Cloud微服务和分布式系统实践</a><time datetime="2023-10-13T10:57:16.000Z" title="发表于 2023-10-13 18:57:16">2023-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/Spring-Boot%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/" title="Spring Boot从入门到实战"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spring Boot从入门到实战"/></a><div class="content"><a class="title" href="/2023/10/09/Spring-Boot%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/" title="Spring Boot从入门到实战">Spring Boot从入门到实战</a><time datetime="2023-10-09T07:04:53.000Z" title="发表于 2023-10-09 15:04:53">2023-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/07/Java%E7%BC%96%E7%A8%8B%E7%9A%84%E9%80%BB%E8%BE%91/" title="Java编程的逻辑"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java编程的逻辑"/></a><div class="content"><a class="title" href="/2023/10/07/Java%E7%BC%96%E7%A8%8B%E7%9A%84%E9%80%BB%E8%BE%91/" title="Java编程的逻辑">Java编程的逻辑</a><time datetime="2023-10-07T08:55:18.000Z" title="发表于 2023-10-07 16:55:18">2023-10-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By dpWu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>