<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>how to train a state-of-the-art model | 平博社</title><meta name="keywords" content="深度学习"><meta name="author" content="dpWu"><meta name="copyright" content="dpWu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="tips： 想要进行模型的构建，fastai并没有提供具体的API，所以想要实现自定义模型需要通过pytorch的接口实现   Mixup：一个有力的数据增强技术 重新调整图片大小  TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法  当使用预训练好的模型（transfer learning）时抑或是从头开始，可以">
<meta property="og:type" content="article">
<meta property="og:title" content="how to train a state-of-the-art model">
<meta property="og:url" content="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/index.html">
<meta property="og:site_name" content="平博社">
<meta property="og:description" content="tips： 想要进行模型的构建，fastai并没有提供具体的API，所以想要实现自定义模型需要通过pytorch的接口实现   Mixup：一个有力的数据增强技术 重新调整图片大小  TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法  当使用预训练好的模型（transfer learning）时抑或是从头开始，可以">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-05-15T11:32:49.000Z">
<meta property="article:modified_time" content="2023-09-13T02:00:41.941Z">
<meta property="article:author" content="dpWu">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/assets/avatar.jpeg"><link rel="canonical" href="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'how to train a state-of-the-art model',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-13 10:00:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">平博社</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">how to train a state-of-the-art model</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-15T11:32:49.000Z" title="发表于 2023-05-15 19:32:49">2023-05-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-13T02:00:41.941Z" title="更新于 2023-09-13 10:00:41">2023-09-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="how to train a state-of-the-art model"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>tips： 想要进行模型的构建，fastai并没有提供具体的API，所以想要实现自定义模型需要通过pytorch的接口实现</p>
</blockquote>
<ul>
<li>Mixup：一个有力的数据增强技术</li>
<li>重新调整图片大小 </li>
<li>TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法</li>
</ul>
<h1 id="当使用预训练好的模型（transfer-learning）时抑或是从头开始，可以使用如下技术，这是非常重要的"><a href="#当使用预训练好的模型（transfer-learning）时抑或是从头开始，可以使用如下技术，这是非常重要的" class="headerlink" title="当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的"></a>当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的</h1><blockquote>
<p>在第7章节</p>
</blockquote>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>在使用预训练模型时，会默认使用ImageNet的平均值和方差</p>
<p>而从头开始训练模型时，需要手动添加平均值和方差</p>
<h2 id="Progressive-Resizing"><a href="#Progressive-Resizing" class="headerlink" title="Progressive Resizing"></a>Progressive Resizing</h2><p>什么时Progressive Resizing</p>
<p>答：一般图片训练时大小是224x224. 我们可以一开始使用较小的图片尺寸进行训练，然后慢慢的提高到224. 这就是 Progressive Resizing</p>
<p>技巧就是：</p>
<ol>
<li>开始使用较小的图片进行训练，然后越往后训练，图片大小逐渐变大。</li>
<li>花费较多的epoch放在小图片训练上，因为这样可以使得训练的更快</li>
</ol>
<p>众所周知：浅层的网络一般学习 像 诸如：edges and gradients，而深层的网络一般学习像 诸如： noses and sunsets</p>
<p>Progressive Resizing其实也是另一种形式的数据增强</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152034725.png" alt="image-20230515203431664"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152034723.png" alt="image-20230515203452673"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152035677.png" alt="image-20230515203514606"></p>
<h2 id="Test-Time-Augmentation（TTA）"><a href="#Test-Time-Augmentation（TTA）" class="headerlink" title="Test Time Augmentation（TTA）"></a>Test Time Augmentation（TTA）</h2><blockquote>
<p>什么是TTA：</p>
<p>答：During inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p>
</blockquote>
<p>当我们使用random cropping，fastai会自动对validation set使用center cropping。</p>
<p>这对于一些问题，使用random cropping就不好。所以我们应该尽量避免使用random cropping</p>
<ul>
<li>对于解决random cropping的问题：<ul>
<li>方案一：we could simply squish or stretch the rectangular images to fit into a square space. But then we miss out on a very useful data augmentation, and we also make the image recognition more difficult for our model, because it has to learn how to recognize squished and squeezed images, rather than just correctly proportioned images.</li>
<li>方案二：Another solution is to not center crop for validation, but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmen‐ tation parameters. This is known as test time augmentation (TTA)</li>
</ul>
</li>
</ul>
<p>fastai 默认会使用一个未增强的中心裁剪的图像和4个随机增强的图像进行TTA。默认使用验证集而不是测试集</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152055662.png" alt="image-20230515205544638"></p>
<p>在我看来：TTA给予了一个非常好的性能表现，并且不需要训练的任何要求，但是会使得预测变得慢点，幸运的是这些延迟可以接受</p>
<h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>具体实现如下：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152102959.png" alt="image-20230515210217881"></p>
<p>对于以上实现有一些前提条件：</p>
<ol>
<li>targets need to be one-hot encoded.</li>
</ol>
<p>Mixup例子：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152104034.png" alt="image-20230515210457819"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152107698.png" alt="image-20230515210716632"></p>
<p>但是使用了Mixup之后，model就很难train，并且预测都是需要预测两个label。所以需要更大的epoch。一般需要epoch&gt;80</p>
<h2 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h2><p>label smoothing will make your training more robust, even if there is mislabeled data</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161008822.png" alt="image-20230516100804734"></p>
<p>就想Mixup一样，需要train更多的epochs，才能有比较好的的提升</p>
<h1 id="learning-rate-finder"><a href="#learning-rate-finder" class="headerlink" title="learning rate finder"></a>learning rate finder</h1><blockquote>
<p>在第五章节</p>
<p>fastai 默认使用lr=1e-3</p>
</blockquote>
<ul>
<li>怎样取lr？<ul>
<li>取在达到最小loss的lr小一个数量级的lr</li>
<li>loss降低的速度快的点的lr</li>
</ul>
</li>
</ul>
<p>我们一般选取的是valley点，也就是loss降低的速度快的点的lr</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161146433.png" alt="image-20230516114624366"></p>
<h1 id="fine-tune"><a href="#fine-tune" class="headerlink" title="fine_tune"></a>fine_tune</h1><p>当我们使用一个预训练模型， fastai会自动冻结预训练所包含的层的参数，当我们使用fine_tune时，会做两件事情：</p>
<ul>
<li>训练你自定义的层并且只训练一个epoch，冻结其他预训练层</li>
<li>解冻所有层，然后训练所要求的epoch数</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161516108.png" alt="image-20230516151646081"></p>
<h1 id="fit-one-cycle"><a href="#fit-one-cycle" class="headerlink" title="fit_one_cycle"></a>fit_one_cycle</h1><p> 因为当我们使用一个预训练模型， fastai会自动冻结预训练所包含的层的参数，所以fit_one_cycle是只训练你自定义加的层。</p>
<p>并且默认是一开始lr比较小，然后逐渐增加，然后又逐渐降低</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161516434.png" alt="image-20230516151629380"></p>
<p>也就是说你可以在fit_one_cycle的时候设置一个学习率的最大上限，然后在训练的时候，学习率在每个batch就进行改变（注意不是每个epoch），学习率的转化是从lr_max/div 到 lr_max 最后到lr_max/div_final。</p>
<blockquote>
<p>注：learn.fit_one_cycle(2)和learn.fit_one_cycle(2，slice(1e-4, 1e-3))好像没什么区别，所以slice（）无用</p>
</blockquote>
<h1 id="Discriminative-Learning-Rates"><a href="#Discriminative-Learning-Rates" class="headerlink" title="Discriminative Learning Rates"></a>Discriminative Learning Rates</h1><blockquote>
<p>fastai 默认方法就是使用这个Discriminative Learning Rates</p>
<p>Discriminative Learning Rates意思就是不同深度的层的lr不同</p>
<p>也就是说，浅层的lr小，深层的lr大</p>
</blockquote>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161248844.png" alt="image-20230516124856734"></p>
<p>第一层的lr=1e-6，最后一层的lr=1e-4</p>
<h1 id="Selecting-the-Number-of-Epochs"><a href="#Selecting-the-Number-of-Epochs" class="headerlink" title="Selecting the Number of Epochs"></a>Selecting the Number of Epochs</h1><p>先试着训练几次epoch，然后查看train和validation loss，和你的metrics（如 accuracy），如果在最后一个epoch都是在getting better，那么可以多训练几个epochs</p>
<p>重点关注 metrics，val loss 不是很重要。因为在一开始训练的时候，val loss 肯定会比较大，因为模型这个时候是过于自信的。loss函数只是我们用来进行优化的。</p>
<h1 id="回调"><a href="#回调" class="headerlink" title="回调"></a>回调</h1><p>具体看这篇博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhouchen1998/article/details/90071837">https://blog.csdn.net/zhouchen1998/article/details/90071837</a></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305181046137.png" alt="image-20230518104629003"></p>
<h1 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h1><p>learn.model.state_dict()：取模型参数</p>
<p>torch.save(learn.model.state_dict(), ‘test.pth’)  保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn2 = vision_learner(data, resnet18, metrics=[accuracy])</span><br><span class="line">learn2.model.load_state_dict(torch.load(<span class="string">&#x27;test.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h1 id="修改模型"><a href="#修改模型" class="headerlink" title="修改模型"></a>修改模型</h1><ul>
<li>增加：learn.model.add_module(‘encode_1’,nn.Conv2d(512, 32, kernel_size=2, stride=2))</li>
<li>修改(此例中写改了bias从False修改为True)：learn2.model[0][0] = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias= True)</li>
</ul>
<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>dls2 = ImageDataLoaders.from_folder(p2, num_workers=0) # 2分类</p>
<p>learn2.predict(dls2.train_ds[10000][0])</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict的输出：(&#x27;7&#x27;, tensor(1), tensor([3.5690e-06, 9.9931e-01, 6.8716e-04]))</span><br></pre></td></tr></table></figure>
<h1 id="fastai-vision-models"><a href="#fastai-vision-models" class="headerlink" title="fastai.vision.models"></a>fastai.vision.models</h1><p>Fastai中模型都是基于预定义的一些模型，这些模型都在 fastai.vision.models 下，是对torchvision定义的一些模型结构的引用和完善。</p>
<p>一些使用pytorch实现的常用模型</p>
<p>链接如下：<a target="_blank" rel="noopener" href="https://github.com/osmr/imgclsmob/tree/master/pytorch/pytorchcv/models">https://github.com/osmr/imgclsmob/tree/master/pytorch/pytorchcv/models</a></p>
<h1 id="怎么将pytorch模型应用到fastai中"><a href="#怎么将pytorch模型应用到fastai中" class="headerlink" title="怎么将pytorch模型应用到fastai中"></a>怎么将pytorch模型应用到fastai中</h1><ol>
<li><p>自定义模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span> ,kernel_size = <span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">128</span>,<span class="number">25</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net() <span class="comment"># 实例化模型</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>```python</p>
<h1 id="获取learn对象的函数，arch表示我自定义的模型模型"><a href="#获取learn对象的函数，arch表示我自定义的模型模型" class="headerlink" title="获取learn对象的函数，arch表示我自定义的模型模型"></a>获取learn对象的函数，arch表示我自定义的模型模型</h1><p>def get_learn(arch):</p>
<pre><code>x  = &#39;D:/数据集/malimg_dataset/malimg_paper_dataset_imgs&#39;
path = Path(x)
data = ImageDataLoaders.from_folder(path, train=&#39;.&#39;, valid_pct=0.2, item_tfms=Resize(224),                 batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)], num_workers=4, device=&#39;cuda&#39;,         seed=42)
# 不能使用vision_learner,会报错，需要使用Learner()
#learn = vision_learner(data, models.resnet18, metrics=[accuracy], model_dir = Path(&#39;./models&#39;),path =         Path(&quot;.&quot;))
learn = Learner(data, arch, metrics=[accuracy], model_dir = Path(&#39;./models&#39;),path = Path(&quot;.&quot;))
return learn
</code></pre><h1 id="实例化learn"><a href="#实例化learn" class="headerlink" title="实例化learn"></a>实例化learn</h1><p>learn = get_learn(net)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. ```python</span><br><span class="line">   # 修改网络结构</span><br><span class="line">   learn.model[1] = nn.Sequential(nn.AdaptiveAvgPool2d(output_size=(1, 1)),</span><br><span class="line">                                  Flatten(full=False),</span><br><span class="line">                                 nn.Linear(512, 25, bias=False))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305201358427.png" alt="image-20230520135822272"></p>
<h1 id="怎么查看loss，lr变化"><a href="#怎么查看loss，lr变化" class="headerlink" title="怎么查看loss，lr变化"></a>怎么查看loss，lr变化</h1><ol>
<li><p>观察学习率随着迭代次数的变化：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.recorder.plot_sched()</span><br></pre></td></tr></table></figure>
</li>
<li><p>观察损失：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.recorder.plot_loss()</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以使用lr_find()找到损失仍在明显改善最高学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#learn.lr_find()</span></span><br><span class="line"><span class="comment">#learn.recorder.plot()</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="将训练好的模型参数保存好"><a href="#将训练好的模型参数保存好" class="headerlink" title="将训练好的模型参数保存好"></a>将训练好的模型参数保存好</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存网络参数参数，f表示存储位置</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_state_dict</span>(<span class="params">learn, f</span>):</span><br><span class="line">    torch.save(learn.model.state_dict(), f)</span><br><span class="line"><span class="comment"># 加载网络参数    </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_state_dict</span>(<span class="params">learn, f</span>):</span><br><span class="line">    learn.model.load_state_dict(torch.load(f))</span><br><span class="line">    <span class="keyword">return</span> learn</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_learn</span>(<span class="params">arch</span>):</span><br><span class="line">    x  = <span class="string">&#x27;D:/数据集/malimg_dataset/malimg_paper_dataset_imgs&#x27;</span></span><br><span class="line">    path = Path(x)</span><br><span class="line">    data = ImageDataLoaders.from_folder(path, train=<span class="string">&#x27;.&#x27;</span>, valid_pct=<span class="number">0.2</span>, item_tfms=Resize(<span class="number">224</span>), batch_tfms=[*aug_transforms(), Normalize.from_stats(*imagenet_stats)], num_workers=<span class="number">4</span>, device=<span class="string">&#x27;cuda&#x27;</span>, seed=<span class="number">42</span>)</span><br><span class="line"><span class="comment">#     learn = vision_learner(data, arch, metrics=[accuracy], model_dir = Path(&#x27;./models&#x27;),path = Path(&quot;.&quot;))</span></span><br><span class="line">    learn = Learner(data, arch, metrics=[accuracy], model_dir = Path(<span class="string">&#x27;./models&#x27;</span>),path = Path(<span class="string">&quot;.&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> learn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span> ,kernel_size = <span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">128</span>,<span class="number">25</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<p>learn = get_learn(net)   注意这个arch必须是之前保存的pth文件相同的网络 如 Net上面定义的网络架构</p>
<p>learn load_state_dict(learn, ‘test.pth’)</p>
<h2 id="更简单的方法"><a href="#更简单的方法" class="headerlink" title="更简单的方法"></a>更简单的方法</h2><p>learn.save(‘10epoch’)</p>
<p>learn = learn.load(‘10epoch’)</p>
<h1 id="freeze和unfreeze"><a href="#freeze和unfreeze" class="headerlink" title="freeze和unfreeze"></a>freeze和unfreeze</h1><p>在不使用freeze和unfreeze方法下，使用 fit_one_cycle方法是会将网络的所有参数进行更新</p>
<p>在使用learn.freeze()后，相当于所有层的参数都不更新了</p>
<p>使用自定义模型 这两个方法就失效了</p>
<p>需要使用torch提供的预训练模型，这两个方法才不失效，</p>
<p>其实这样也很合理，因为你自定义模型就是为了从头开始训练啊。</p>
<h1 id="怎么查看learner使用的损失函数等"><a href="#怎么查看learner使用的损失函数等" class="headerlink" title="怎么查看learner使用的损失函数等"></a>怎么查看learner使用的损失函数等</h1><p>查看learn的源码</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://wdpname.github.io">dpWu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/">https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wdpname.github.io" target="_blank">平博社</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/21/Depthwise-%E5%8D%B7%E7%A7%AF-%EF%BC%8CPointwise-%E5%8D%B7%E7%A7%AF%E4%B8%8E%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8C%BA%E5%88%AB/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Depthwise 卷积 ，Pointwise 卷积与普通卷积的区别</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/15/ConvNext-Block%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ConvNext Block的模型创建</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/05/15/BN%E5%92%8CLN%E7%9A%84%E5%8C%BA%E5%88%AB/" title="BN和LN的区别"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-15</div><div class="title">BN和LN的区别</div></div></a></div><div><a href="/2022/10/16/CNN/" title="CNN"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-16</div><div class="title">CNN</div></div></a></div><div><a href="/2023/05/21/Depthwise-%E5%8D%B7%E7%A7%AF-%EF%BC%8CPointwise-%E5%8D%B7%E7%A7%AF%E4%B8%8E%E6%99%AE%E9%80%9A%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8C%BA%E5%88%AB/" title="Depthwise 卷积 ，Pointwise 卷积与普通卷积的区别"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-21</div><div class="title">Depthwise 卷积 ，Pointwise 卷积与普通卷积的区别</div></div></a></div><div><a href="/2022/11/24/RNN/" title="RNN"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-24</div><div class="title">RNN</div></div></a></div><div><a href="/2023/05/15/TTA/" title="TTA"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-15</div><div class="title">TTA</div></div></a></div><div><a href="/2023/01/10/pytorch%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B0%8F%E5%9C%9F%E5%A0%86%EF%BC%89/" title="pytorch基础（小土堆）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-10</div><div class="title">pytorch基础（小土堆）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dpWu</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdpname" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://1939317922@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BD%93%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%88transfer-learning%EF%BC%89%E6%97%B6%E6%8A%91%E6%88%96%E6%98%AF%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E5%A6%82%E4%B8%8B%E6%8A%80%E6%9C%AF%EF%BC%8C%E8%BF%99%E6%98%AF%E9%9D%9E%E5%B8%B8%E9%87%8D%E8%A6%81%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Normalization"><span class="toc-number">1.1.</span> <span class="toc-text">Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Progressive-Resizing"><span class="toc-number">1.2.</span> <span class="toc-text">Progressive Resizing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Test-Time-Augmentation%EF%BC%88TTA%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">Test Time Augmentation（TTA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mixup"><span class="toc-number">1.4.</span> <span class="toc-text">Mixup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#label-smoothing"><span class="toc-number">1.5.</span> <span class="toc-text">label smoothing</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#learning-rate-finder"><span class="toc-number">2.</span> <span class="toc-text">learning rate finder</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fine-tune"><span class="toc-number">3.</span> <span class="toc-text">fine_tune</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fit-one-cycle"><span class="toc-number">4.</span> <span class="toc-text">fit_one_cycle</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Discriminative-Learning-Rates"><span class="toc-number">5.</span> <span class="toc-text">Discriminative Learning Rates</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Selecting-the-Number-of-Epochs"><span class="toc-number">6.</span> <span class="toc-text">Selecting the Number of Epochs</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9E%E8%B0%83"><span class="toc-number">7.</span> <span class="toc-text">回调</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">模型参数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.</span> <span class="toc-text">修改模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">10.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fastai-vision-models"><span class="toc-number">11.</span> <span class="toc-text">fastai.vision.models</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E5%B0%86pytorch%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%B0fastai%E4%B8%AD"><span class="toc-number">12.</span> <span class="toc-text">怎么将pytorch模型应用到fastai中</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96learn%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%8Carch%E8%A1%A8%E7%A4%BA%E6%88%91%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">13.</span> <span class="toc-text">获取learn对象的函数，arch表示我自定义的模型模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96learn"><span class="toc-number">14.</span> <span class="toc-text">实例化learn</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E6%9F%A5%E7%9C%8Bloss%EF%BC%8Clr%E5%8F%98%E5%8C%96"><span class="toc-number">15.</span> <span class="toc-text">怎么查看loss，lr变化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%86%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BF%9D%E5%AD%98%E5%A5%BD"><span class="toc-number">16.</span> <span class="toc-text">将训练好的模型参数保存好</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E7%AE%80%E5%8D%95%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">16.1.</span> <span class="toc-text">更简单的方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#freeze%E5%92%8Cunfreeze"><span class="toc-number">17.</span> <span class="toc-text">freeze和unfreeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E6%9F%A5%E7%9C%8Blearner%E4%BD%BF%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%AD%89"><span class="toc-number">18.</span> <span class="toc-text">怎么查看learner使用的损失函数等</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/05/JavaScript%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%EF%BC%88%E7%AC%AC%E4%B8%89%E7%89%88%EF%BC%89/" title="JavaScript高级程序设计（第三版）"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaScript高级程序设计（第三版）"/></a><div class="content"><a class="title" href="/2023/10/05/JavaScript%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%EF%BC%88%E7%AC%AC%E4%B8%89%E7%89%88%EF%BC%89/" title="JavaScript高级程序设计（第三版）">JavaScript高级程序设计（第三版）</a><time datetime="2023-10-05T14:44:18.000Z" title="发表于 2023-10-05 22:44:18">2023-10-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/15/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%8A%80%E5%B7%A7/" title="二叉树的技巧"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="二叉树的技巧"/></a><div class="content"><a class="title" href="/2023/08/15/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%8A%80%E5%B7%A7/" title="二叉树的技巧">二叉树的技巧</a><time datetime="2023-08-15T04:23:33.000Z" title="发表于 2023-08-15 12:23:33">2023-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/14/%E9%93%BE%E8%A1%A8%E7%9A%84%E6%8A%80%E5%B7%A7/" title="链表的技巧"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="链表的技巧"/></a><div class="content"><a class="title" href="/2023/08/14/%E9%93%BE%E8%A1%A8%E7%9A%84%E6%8A%80%E5%B7%A7/" title="链表的技巧">链表的技巧</a><time datetime="2023-08-14T04:03:55.000Z" title="发表于 2023-08-14 12:03:55">2023-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/21/7-21/" title="7-21"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="7-21"/></a><div class="content"><a class="title" href="/2023/07/21/7-21/" title="7-21">7-21</a><time datetime="2023-07-21T03:07:00.000Z" title="发表于 2023-07-21 11:07:00">2023-07-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/20/7-20/" title="7-20"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="7-20"/></a><div class="content"><a class="title" href="/2023/07/20/7-20/" title="7-20">7-20</a><time datetime="2023-07-20T01:32:39.000Z" title="发表于 2023-07-20 09:32:39">2023-07-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By dpWu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>