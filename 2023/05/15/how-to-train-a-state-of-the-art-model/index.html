<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>how to train a state-of-the-art model | 平博社</title><meta name="keywords" content="深度学习"><meta name="author" content="dpWu"><meta name="copyright" content="dpWu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="Mixup：一个有力的数据增强技术 重新调整图片大小  TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法  当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的 在第7章节  Normalization在使用预训练模型时，会默认使用ImageNet的平均值和方">
<meta property="og:type" content="article">
<meta property="og:title" content="how to train a state-of-the-art model">
<meta property="og:url" content="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/index.html">
<meta property="og:site_name" content="平博社">
<meta property="og:description" content="Mixup：一个有力的数据增强技术 重新调整图片大小  TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法  当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的 在第7章节  Normalization在使用预训练模型时，会默认使用ImageNet的平均值和方">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-05-15T11:32:49.000Z">
<meta property="article:modified_time" content="2023-05-16T07:46:43.923Z">
<meta property="article:author" content="dpWu">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/assets/avatar.jpeg"><link rel="canonical" href="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'how to train a state-of-the-art model',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-16 15:46:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/assets/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">127</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">平博社</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">how to train a state-of-the-art model</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-15T11:32:49.000Z" title="发表于 2023-05-15 19:32:49">2023-05-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-16T07:46:43.923Z" title="更新于 2023-05-16 15:46:43">2023-05-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="how to train a state-of-the-art model"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><ul>
<li>Mixup：一个有力的数据增强技术</li>
<li>重新调整图片大小 </li>
<li>TTA：test time augmentation：一个在预测阶段，将一个图片进行数据增强，然后分别预测，投票选出预测分类的方法</li>
</ul>
<h1 id="当使用预训练好的模型（transfer-learning）时抑或是从头开始，可以使用如下技术，这是非常重要的"><a href="#当使用预训练好的模型（transfer-learning）时抑或是从头开始，可以使用如下技术，这是非常重要的" class="headerlink" title="当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的"></a>当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的</h1><blockquote>
<p>在第7章节</p>
</blockquote>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>在使用预训练模型时，会默认使用ImageNet的平均值和方差</p>
<p>而从头开始训练模型时，需要手动添加平均值和方差</p>
<h2 id="Progressive-Resizing"><a href="#Progressive-Resizing" class="headerlink" title="Progressive Resizing"></a>Progressive Resizing</h2><p>什么时Progressive Resizing</p>
<p>答：一般图片训练时大小是224x224. 我们可以一开始使用较小的图片尺寸进行训练，然后慢慢的提高到224. 这就是 Progressive Resizing</p>
<p>技巧就是：</p>
<ol>
<li>开始使用较小的图片进行训练，然后越往后训练，图片大小逐渐变大。</li>
<li>花费较多的epoch放在小图片训练上，因为这样可以使得训练的更快</li>
</ol>
<p>众所周知：浅层的网络一般学习 像 诸如：edges and gradients，而深层的网络一般学习像 诸如： noses and sunsets</p>
<p>Progressive Resizing其实也是另一种形式的数据增强</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152034725.png" alt="image-20230515203431664"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152034723.png" alt="image-20230515203452673"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152035677.png" alt="image-20230515203514606"></p>
<h2 id="Test-Time-Augmentation（TTA）"><a href="#Test-Time-Augmentation（TTA）" class="headerlink" title="Test Time Augmentation（TTA）"></a>Test Time Augmentation（TTA）</h2><blockquote>
<p>什么是TTA：</p>
<p>答：During inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image</p>
</blockquote>
<p>当我们使用random cropping，fastai会自动对validation set使用center cropping。</p>
<p>这对于一些问题，使用random cropping就不好。所以我们应该尽量避免使用random cropping</p>
<ul>
<li>对于解决random cropping的问题：<ul>
<li>方案一：we could simply squish or stretch the rectangular images to fit into a square space. But then we miss out on a very useful data augmentation, and we also make the image recognition more difficult for our model, because it has to learn how to recognize squished and squeezed images, rather than just correctly proportioned images.</li>
<li>方案二：Another solution is to not center crop for validation, but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmen‐ tation parameters. This is known as test time augmentation (TTA)</li>
</ul>
</li>
</ul>
<p>fastai 默认会使用一个未增强的中心裁剪的图像和4个随机增强的图像进行TTA。默认使用验证集而不是测试集</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152055662.png" alt="image-20230515205544638"></p>
<p>在我看来：TTA给予了一个非常好的性能表现，并且不需要训练的任何要求，但是会使得预测变得慢点，幸运的是这些延迟可以接受</p>
<h2 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h2><p>具体实现如下：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152102959.png" alt="image-20230515210217881"></p>
<p>对于以上实现有一些前提条件：</p>
<ol>
<li>targets need to be one-hot encoded.</li>
</ol>
<p>Mixup例子：</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152104034.png" alt="image-20230515210457819"></p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305152107698.png" alt="image-20230515210716632"></p>
<p>但是使用了Mixup之后，model就很难train，并且预测都是需要预测两个label。所以需要更大的epoch。一般需要epoch&gt;80</p>
<h2 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h2><p>label smoothing will make your training more robust, even if there is mislabeled data</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161008822.png" alt="image-20230516100804734"></p>
<p>就想Mixup一样，需要train更多的epochs，才能有比较好的的提升</p>
<h1 id="learning-rate-finder"><a href="#learning-rate-finder" class="headerlink" title="learning rate finder"></a>learning rate finder</h1><blockquote>
<p>在第五章节</p>
<p>fastai 默认使用lr=1e-3</p>
</blockquote>
<ul>
<li>怎样取lr？<ul>
<li>取在达到最小loss的lr小一个数量级的lr</li>
<li>loss降低的速度快的点的lr</li>
</ul>
</li>
</ul>
<p>我没一般选取的是valley点，也就是loss降低的速度快的点的lr</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161146433.png" alt="image-20230516114624366"></p>
<h1 id="fine-tune"><a href="#fine-tune" class="headerlink" title="fine_tune"></a>fine_tune</h1><p>当我们使用一个预训练模型， fastai会自动冻结预训练所包含的层的参数，当我们使用fine_tune时，会做两件事情：</p>
<ul>
<li>训练你自定义的层并且只训练一个epoch，冻结其他预训练层</li>
<li>解冻所有层，然后训练所要求的epoch数</li>
</ul>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161516108.png" alt="image-20230516151646081"></p>
<h1 id="fit-one-cycle"><a href="#fit-one-cycle" class="headerlink" title="fit_one_cycle"></a>fit_one_cycle</h1><p> 因为当我们使用一个预训练模型， fastai会自动冻结预训练所包含的层的参数，所以fit_one_cycle是只训练你自定义加的层。</p>
<p>并且默认是一开始lr比较小，然后逐渐增加，然后又逐渐降低</p>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161516434.png" alt="image-20230516151629380"></p>
<p>也就是说你可以在fit_one_cycle的时候设置一个学习率的最大上限，然后在训练的时候，学习率在每个batch就进行改变（注意不是每个epoch），学习率的转化是从lr_max/div 到 lr_max 最后到lr_max/div_final。</p>
<h1 id="Discriminative-Learning-Rates"><a href="#Discriminative-Learning-Rates" class="headerlink" title="Discriminative Learning Rates"></a>Discriminative Learning Rates</h1><blockquote>
<p>fastai 默认方法就是使用这个Discriminative Learning Rates</p>
<p>Discriminative Learning Rates意思就是不同深度的层的lr不同</p>
<p>也就是说，浅层的lr小，深层的lr大</p>
</blockquote>
<p><img src="https://gitee.com/fjkf/images/raw/master/202305161248844.png" alt="image-20230516124856734"></p>
<p>第一层的lr=1e-6，最后一层的lr=1e-4</p>
<h1 id="Selecting-the-Number-of-Epochs"><a href="#Selecting-the-Number-of-Epochs" class="headerlink" title="Selecting the Number of Epochs"></a>Selecting the Number of Epochs</h1><p>先试着训练几次epoch，然后查看train和validation loss，和你的metrics（如 accuracy），如果在最后一个epoch都是在getting better，那么可以多训练几个epochs</p>
<p>重点关注 metrics，val loss 不是很重要。因为在一开始训练的时候，val loss 肯定会比较大，因为模型这个时候是过于自信的。loss函数只是我们用来进行优化的。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://wdpname.github.io">dpWu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/">https://wdpname.github.io/2023/05/15/how-to-train-a-state-of-the-art-model/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wdpname.github.io" target="_blank">平博社</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/05/15/ConvNext-Block%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ConvNext Block的模型创建</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/16/CNN/" title="CNN"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-16</div><div class="title">CNN</div></div></a></div><div><a href="/2023/05/15/BN%E5%92%8CLN%E7%9A%84%E5%8C%BA%E5%88%AB/" title="BN和LN的区别"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-15</div><div class="title">BN和LN的区别</div></div></a></div><div><a href="/2022/11/24/RNN/" title="RNN"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-24</div><div class="title">RNN</div></div></a></div><div><a href="/2023/05/15/TTA/" title="TTA"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-15</div><div class="title">TTA</div></div></a></div><div><a href="/2022/11/24/self-attention%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="self-attention自注意力机制"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-24</div><div class="title">self-attention自注意力机制</div></div></a></div><div><a href="/2023/01/10/pytorch%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B0%8F%E5%9C%9F%E5%A0%86%EF%BC%89/" title="pytorch基础（小土堆）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-10</div><div class="title">pytorch基础（小土堆）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/assets/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">dpWu</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">127</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdpname" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://1939317922@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BD%93%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%88transfer-learning%EF%BC%89%E6%97%B6%E6%8A%91%E6%88%96%E6%98%AF%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E5%A6%82%E4%B8%8B%E6%8A%80%E6%9C%AF%EF%BC%8C%E8%BF%99%E6%98%AF%E9%9D%9E%E5%B8%B8%E9%87%8D%E8%A6%81%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">当使用预训练好的模型（transfer learning）时抑或是从头开始，可以使用如下技术，这是非常重要的</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Normalization"><span class="toc-number">1.1.</span> <span class="toc-text">Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Progressive-Resizing"><span class="toc-number">1.2.</span> <span class="toc-text">Progressive Resizing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Test-Time-Augmentation%EF%BC%88TTA%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">Test Time Augmentation（TTA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mixup"><span class="toc-number">1.4.</span> <span class="toc-text">Mixup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#label-smoothing"><span class="toc-number">1.5.</span> <span class="toc-text">label smoothing</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#learning-rate-finder"><span class="toc-number">2.</span> <span class="toc-text">learning rate finder</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fine-tune"><span class="toc-number">3.</span> <span class="toc-text">fine_tune</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fit-one-cycle"><span class="toc-number">4.</span> <span class="toc-text">fit_one_cycle</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Discriminative-Learning-Rates"><span class="toc-number">5.</span> <span class="toc-text">Discriminative Learning Rates</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Selecting-the-Number-of-Epochs"><span class="toc-number">6.</span> <span class="toc-text">Selecting the Number of Epochs</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/15/how-to-train-a-state-of-the-art-model/" title="how to train a state-of-the-art model"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="how to train a state-of-the-art model"/></a><div class="content"><a class="title" href="/2023/05/15/how-to-train-a-state-of-the-art-model/" title="how to train a state-of-the-art model">how to train a state-of-the-art model</a><time datetime="2023-05-15T11:32:49.000Z" title="发表于 2023-05-15 19:32:49">2023-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/15/ConvNext-Block%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA/" title="ConvNext Block的模型创建"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ConvNext Block的模型创建"/></a><div class="content"><a class="title" href="/2023/05/15/ConvNext-Block%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%88%9B%E5%BB%BA/" title="ConvNext Block的模型创建">ConvNext Block的模型创建</a><time datetime="2023-05-15T10:44:32.000Z" title="发表于 2023-05-15 18:44:32">2023-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/15/BN%E5%92%8CLN%E7%9A%84%E5%8C%BA%E5%88%AB/" title="BN和LN的区别"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="BN和LN的区别"/></a><div class="content"><a class="title" href="/2023/05/15/BN%E5%92%8CLN%E7%9A%84%E5%8C%BA%E5%88%AB/" title="BN和LN的区别">BN和LN的区别</a><time datetime="2023-05-15T10:15:20.000Z" title="发表于 2023-05-15 18:15:20">2023-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/15/TTA/" title="TTA"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TTA"/></a><div class="content"><a class="title" href="/2023/05/15/TTA/" title="TTA">TTA</a><time datetime="2023-05-15T07:42:04.000Z" title="发表于 2023-05-15 15:42:04">2023-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/07/MORE-CONVNETS-IN-THE-2020S-SCALING-UP-KERNELS-BEYOND-51-%C3%97-51-USING-SPARSITY/" title="MORE_CONVNETS_IN_THE_2020S_SCALING_UP_KERNELS BEYOND 51 × 51 USING SPARSITY"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MORE_CONVNETS_IN_THE_2020S_SCALING_UP_KERNELS BEYOND 51 × 51 USING SPARSITY"/></a><div class="content"><a class="title" href="/2023/05/07/MORE-CONVNETS-IN-THE-2020S-SCALING-UP-KERNELS-BEYOND-51-%C3%97-51-USING-SPARSITY/" title="MORE_CONVNETS_IN_THE_2020S_SCALING_UP_KERNELS BEYOND 51 × 51 USING SPARSITY">MORE_CONVNETS_IN_THE_2020S_SCALING_UP_KERNELS BEYOND 51 × 51 USING SPARSITY</a><time datetime="2023-05-07T10:54:08.000Z" title="发表于 2023-05-07 18:54:08">2023-05-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By dpWu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>